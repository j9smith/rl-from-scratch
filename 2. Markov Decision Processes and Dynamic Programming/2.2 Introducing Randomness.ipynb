{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Markov Decision Processes\n",
    "## 2.2 Introducing Randomness\n",
    "\n",
    "- Random transition probabilities $P(s'|s,a)$\n",
    "- Non-deterministic policies $\\pi(a|s)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deterministic Transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S # - - - -\n",
      "↓ # → → → ↓\n",
      "→ → ↑ - # ↓\n",
      "- - - # - F\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "gamma = 0.9\n",
    "\n",
    "grid = np.array([\n",
    "    ['S', '#', '-', '-', '-', '-'],\n",
    "    ['-', '#', '-', '-', '-', '-'],\n",
    "    ['-', '-', '-', '-', '#', '-'],\n",
    "    ['-', '-', '-', '#', '-', 'F']\n",
    "])\n",
    "\n",
    "rows, cols = grid.shape\n",
    "terminal_state = (3, 5)\n",
    "\n",
    "actions = ['up', 'down', 'left', 'right']\n",
    "\n",
    "rewards = { terminal_state: 10}\n",
    "\n",
    "Q_values = { (i, j): {a: 0.0 for a in actions} for i in range(rows) for j in range(cols) }\n",
    "\n",
    "def is_valid(state):\n",
    "    i, j = state\n",
    "    return 0 <= i < rows and 0 <= j < cols and grid[i, j] != '#'\n",
    "\n",
    "def get_next_state(state, action):\n",
    "    i, j = state\n",
    "    if action == 'up': next_state = (i-1, j)\n",
    "    elif action == 'down': next_state = (i+1, j)\n",
    "    elif action == 'left': next_state = (i, j-1)\n",
    "    elif action == 'right': next_state = (i, j+1)\n",
    "    \n",
    "    if is_valid(next_state): return True, next_state\n",
    "    else: return False, state\n",
    "\n",
    "def update():\n",
    "    for state in Q_values.keys():\n",
    "        if state == terminal_state or grid[state] == '#':\n",
    "            continue\n",
    "        for action in actions:\n",
    "            is_valid, next_state = get_next_state(state, action) \n",
    "            if is_valid:\n",
    "                reward = rewards.get(next_state, 0) \n",
    "                Q_values[state][action] = reward + gamma * max(Q_values[next_state].values())\n",
    "            else: \n",
    "                Q_values[state][action] = -5\n",
    "\n",
    "action_symbols = {\n",
    "    \"up\": \"↑\",\n",
    "    \"down\": \"↓\",\n",
    "    \"left\": \"←\",\n",
    "    \"right\": \"→\"\n",
    "}\n",
    "\n",
    "def get_best_action(state):\n",
    "    return max(Q_values[state], key=Q_values[state].get)\n",
    "\n",
    "def visualise_path():\n",
    "    grid_viz = grid.copy()\n",
    "\n",
    "    state = (0, 0)\n",
    "    path = []\n",
    "    max_steps = 1000\n",
    "\n",
    "    for _ in range(max_steps):\n",
    "        path.append(state)\n",
    "        if state == terminal_state:\n",
    "            break\n",
    "\n",
    "        best_action = get_best_action(state)\n",
    "        _, next_state = get_next_state(state, best_action)\n",
    "\n",
    "        if next_state == state:\n",
    "            pass\n",
    "\n",
    "        state = next_state\n",
    "    \n",
    "    for (i, j) in path:\n",
    "        if grid_viz[i, j] not in ['S', 'F']:\n",
    "            grid_viz[i, j] = action_symbols[get_best_action((i, j))]\n",
    "\n",
    "    for row in grid_viz:\n",
    "        print(\" \".join(row))\n",
    "        \n",
    "for i in range(50):\n",
    "    update()\n",
    "\n",
    "visualise_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): {'up': -5, 'down': 3.87, 'left': -5, 'right': -5},\n",
       " (0, 1): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " (0, 2): {'up': -5, 'down': 5.9, 'left': -5, 'right': 5.9},\n",
       " (0, 3): {'up': -5, 'down': 6.56, 'left': 5.31, 'right': 6.56},\n",
       " (0, 4): {'up': -5, 'down': 7.29, 'left': 5.9, 'right': 7.29},\n",
       " (0, 5): {'up': -5, 'down': 8.1, 'left': 6.56, 'right': -5},\n",
       " (1, 0): {'up': 3.49, 'down': 4.3, 'left': -5, 'right': -5},\n",
       " (1, 1): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " (1, 2): {'up': 5.31, 'down': 5.31, 'left': -5, 'right': 6.56},\n",
       " (1, 3): {'up': 5.9, 'down': 5.9, 'left': 5.9, 'right': 7.29},\n",
       " (1, 4): {'up': 6.56, 'down': -5, 'left': 6.56, 'right': 8.1},\n",
       " (1, 5): {'up': 7.29, 'down': 9.0, 'left': 7.29, 'right': -5},\n",
       " (2, 0): {'up': 3.87, 'down': 3.87, 'left': -5, 'right': 4.78},\n",
       " (2, 1): {'up': -5, 'down': 4.3, 'left': 4.3, 'right': 5.31},\n",
       " (2, 2): {'up': 5.9, 'down': 4.78, 'left': 4.78, 'right': 5.9},\n",
       " (2, 3): {'up': 6.56, 'down': -5, 'left': 5.31, 'right': -5},\n",
       " (2, 4): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " (2, 5): {'up': 8.1, 'down': 10.0, 'left': -5, 'right': -5},\n",
       " (3, 0): {'up': 4.3, 'down': -5, 'left': -5, 'right': 4.3},\n",
       " (3, 1): {'up': 4.78, 'down': -5, 'left': 3.87, 'right': 4.78},\n",
       " (3, 2): {'up': 5.31, 'down': -5, 'left': 4.3, 'right': -5},\n",
       " (3, 3): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " (3, 4): {'up': -5, 'down': -5, 'left': -5, 'right': 10.0},\n",
       " (3, 5): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 0.0}}"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_values = {\n",
    "    state: {action: round(value, 2) for action, value in actions.items()}\n",
    "    for state, actions in Q_values.items()\n",
    "}\n",
    "Q_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Deterministic Transitions\n",
    "Recall the formulation of a Markov decision process:\n",
    "$$(\\mathcal{S}, \\mathcal{A}, P, \\mathcal{R}, \\gamma)$$\n",
    "\n",
    "Where: \n",
    "- $\\mathcal{S}$ is the **state space**.\n",
    "- $\\mathcal{A}$ is the **action space**.\n",
    "- $P(s'|s,a)$ is the **transition function** - the probability of reaching state $s'$ by taking action $a$ in state $s$. \n",
    "- $R(s,a,s')$ is the reward received immediately after taking action $a$ in state $s$. \n",
    "- $\\gamma$ is a discount factor.\n",
    "\n",
    "Previously, in our gridworld, we worked by the assumption that state transitions were **completely deterministic**, i.e.:\n",
    "$$P(s'|s,a) = 1 \\quad \\forall s,a \\in \\mathcal{S}, \\mathcal{A}$$\n",
    "\n",
    "This means that every time we took a given action in a given state, we would move to the next state with 100% certainty every single time. For example, if we took action '$\\text{right}$' in state $(0,0)$, our agent will *always* move to state $(0,1)$. \n",
    "\n",
    "In the real world, state transitions are usually not determinstic, but instead are **probabilistic (stochastic)**. \n",
    "\n",
    "Let's go back to our gridworld. This time, the grid is slippery - instead of moving in our intended direction with 100% certainty, there is now a chance of slipping and moving in an unintended direction. \n",
    "\n",
    "Instead, when an action is chosen, we will: \n",
    "- Move in the intended direction with $p=0.8$.\n",
    "- Move left of the intended direction with $p=0.1$.\n",
    "- Move right of the intended direction with $p=0.1$.\n",
    "- Remain in the same state if the action results in an invalid move.\n",
    "\n",
    "Now our **state transition function $P(s'|s,a)$** becomes: \n",
    "$$P(s'|s,a)=\n",
    "\\begin{cases}\n",
    "0.8, & \\text{if } s' \\text{ is the intended direction from } s \\text{ given } a \\\\\n",
    "0.1, & \\text{if } s' \\text{ is to the left of the the intended direction from } s  \\\\\n",
    "0.1, & \\text{if } s' \\text{ is to the right of the intended direction from } s  \\\\\n",
    "0, & \\text{otherwise}\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid(state):\n",
    "    i, j = state\n",
    "    return 0 <= i < rows and 0 <= j < cols and grid[i, j] != '#'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update():\n",
    "    gamma = 0.9\n",
    "    for state in Q_values.keys():\n",
    "\n",
    "        if state == terminal_state or grid[state] == '#':\n",
    "            continue\n",
    "\n",
    "        for action in actions:\n",
    "            is_valid, next_state = get_next_state(state, action)\n",
    "            if is_valid:\n",
    "                reward = rewards.get(next_state, 0)\n",
    "                Q_values[state][action] = reward + gamma * max(Q_values[next_state].values())\n",
    "            else: \n",
    "                Q_values[state][action] = -5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To introduce this stochasticity into our gridworld, we introduce a new function: $\\text{get\\_possible\\_moves}$. This function will return a list of three tuples, with the $0$-th tuple of the list representing the intended action, and the other two tuples representing a perpendicular move to the intended direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_possible_moves(state, action):\n",
    "    i, j = state\n",
    "    if action == 'up':\n",
    "        return [(i-1, j), (i, j-1), (i, j+1)] # up, left, right\n",
    "    elif action == 'down':\n",
    "        return [(i+1, j), (i, j-1), (i, j+1)] # down, left, right\n",
    "    elif action == 'left':\n",
    "        return [(i, j-1), (i-1, j), (i+1, j)] # left, up, down\n",
    "    elif action == 'right':\n",
    "        return [(i, j+1), (i-1, j), (i+1, j)] # right, up, down "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then update our $\\text{get\\_next\\_state}$ function to first sample the actual move from our set of possible moves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "probabilities = [0.8, 0.1, 0.1] # p=0.8 of moving in the intended direction, (1-p)/2 for moving perpendicular\n",
    "\n",
    "def get_next_state(state, action):\n",
    "    possible_moves = get_possible_moves(state, action)\n",
    "\n",
    "    next_state = possible_moves[np.random.choice(3, p=probabilities)]\n",
    "\n",
    "    if is_valid(next_state):\n",
    "        return True, next_state\n",
    "    else: \n",
    "        return False, state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform our updates as before, using the equation: \n",
    "$$Q(s,a)=R + \\gamma \\cdot \\max_{a'}Q(s',a')$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, we just perform one update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can how visualise how an agent following the *optimal policy* $\\pi^*$ (that just means selecting the **best** action in each state every time) navigates the grid after just one update.\n",
    "\n",
    "Recall: \n",
    "$$\\pi^*= \\arg\\max_{a}Q(s,a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this equation to plot our agent's course across the grid. Look closely. We see that in state $(2,3)$ the agent has learned the optimal action '$\\text{left}$'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S # - - - -\n",
      "↓ # - → → ↓\n",
      "→ → → ← # ↓\n",
      "- - - # - F\n"
     ]
    }
   ],
   "source": [
    "visualise_path()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from the previous notebook that the update function iterates over all states and **retrieves the value of the next state corresponding to each action**. It will then use the value of this next state to update its belief about the value of its *own* state. \n",
    "\n",
    "Specifically, we look at this part of the $\\text{update}$ function: \n",
    "```python\n",
    "for action in actions:\n",
    "            is_valid, next_state = get_next_state(state, action)\n",
    "            reward = rewards.get(next_state, 0)\n",
    "                Q_values[state][action] = reward + gamma * max(Q_values[next_state].values())\n",
    "         \n",
    "```\n",
    "Recall how we laid out our state transition probabilities earlier: we said that with probability $0.8$, the action would transition state as intended, but with probability $0.2$, the action would transition state perpendicular to the intended. \n",
    "\n",
    "This means that during our update, the agent tested action '$\\text{left}$' in state $(2,3)$ and, with a probability of 10%, it moved upwards instead (i.e., ended up in state $(1,3)$). It then used the *value of this state* to update its belief about the value of $Q((2,3), \\text{left})$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'up': -5, 'down': -5, 'left': 5.3136, 'right': -5}"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_values[(2,3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do another iteration of our update rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And visualise the path that it has learned now. Already, it is failing to learn an optimal path to reach the terminal state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S # - ↓ ↓ ↓\n",
      "↓ # → → → ↑\n",
      "→ ↑ ↑ - # -\n",
      "- - - # - F\n"
     ]
    }
   ],
   "source": [
    "visualise_path()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the agent gets stuck in state $(1,5)$. Let's look at the Q-values for this state and see what the agent has learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'up': 7.29, 'down': 7.29, 'left': 7.29, 'right': -5}"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_values[(1,5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Q-values for actions '$\\text{up}$', '$\\text{down}$', and '$\\text{left}$' have been assigned equal value. With each value equal, the agent will simply pick the **first occurrence** of the maximum value, and so it selects action '$\\text{up}$'.\n",
    "\n",
    "We can also visualise what happens when we update our Q-values a few more times (remember, we want our Q-values to converge so we can't take the first update at face value):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S # - - - -\n",
      "↑ # - - - -\n",
      "- - - - # -\n",
      "- - - # - F\n"
     ]
    }
   ],
   "source": [
    "for _ in range(50):\n",
    "    update()\n",
    "visualise_path()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent gets stuck almost immediately as the Q-values converge to their 'true' values. This is the problem with taking a deterministic approach when dealing with probabilistic transitions: the Q-values fail to update in a manner that can be useful to us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): {'up': -5, 'down': 3.87, 'left': -5, 'right': -5},\n",
       " (0, 1): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " (0, 2): {'up': -5, 'down': 3.87, 'left': -5, 'right': -5},\n",
       " (0, 3): {'up': -5, 'down': 6.56, 'left': -5, 'right': 6.56},\n",
       " (0, 4): {'up': -5, 'down': 7.29, 'left': 7.29, 'right': 7.29},\n",
       " (0, 5): {'up': -5, 'down': 6.56, 'left': -5, 'right': -5},\n",
       " (1, 0): {'up': 3.49, 'down': 3.49, 'left': 3.49, 'right': -5},\n",
       " (1, 1): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " (1, 2): {'up': 3.49, 'down': 6.56, 'left': -5, 'right': 6.56},\n",
       " (1, 3): {'up': 5.9, 'down': 3.87, 'left': 5.9, 'right': 7.29},\n",
       " (1, 4): {'up': 6.56, 'down': -5, 'left': 6.56, 'right': 6.56},\n",
       " (1, 5): {'up': 5.9, 'down': 9.0, 'left': 5.9, 'right': 5.9},\n",
       " (2, 0): {'up': 3.14, 'down': 3.14, 'left': -5, 'right': 3.87},\n",
       " (2, 1): {'up': -5, 'down': 4.3, 'left': 3.49, 'right': 4.3},\n",
       " (2, 2): {'up': 5.9, 'down': 3.87, 'left': 3.87, 'right': 3.87},\n",
       " (2, 3): {'up': 6.56, 'down': -5, 'left': 5.31, 'right': -5},\n",
       " (2, 4): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " (2, 5): {'up': 8.1, 'down': 10.0, 'left': -5, 'right': -5},\n",
       " (3, 0): {'up': -5, 'down': -5, 'left': -5, 'right': 3.49},\n",
       " (3, 1): {'up': 3.87, 'down': -5, 'left': 3.14, 'right': 3.87},\n",
       " (3, 2): {'up': 5.31, 'down': -5, 'left': -5, 'right': -5},\n",
       " (3, 3): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " (3, 4): {'up': -5, 'down': 10.0, 'left': -5, 'right': 10.0},\n",
       " (3, 5): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 0.0}}"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_values = {\n",
    "    state: {action: round(value, 2) for action, value in actions.items()}\n",
    "    for state, actions in Q_values.items()\n",
    "}\n",
    "Q_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To address this, let's consider our update rule: \n",
    "$$Q(s,a)=R + \\gamma_ \\cdot \\max_{a'} Q(s',a')$$\n",
    "\n",
    "When we update our Q-value using this rule, we assume that the **the action always transitions to the best next state**. This means that the agent is not learning the **true expected value of the next state**. \n",
    "\n",
    "Consider an action which has a 90% chance of a reward of $10$, but a 10% chance of a penalty of $100$. Can we assign a value of $10$ to the action just because it is the most probable outcome? No - **we must weigh all outcomes by their probability** to get an accurate representation of the expected value: \n",
    "$$(0.9 * 10) + (0.1 * -100) = -1$$\n",
    "\n",
    "This is **exactly** what we need to consider when we're navigating our grid. Each action has a probability of transitioning to three separate states: \n",
    "- The one intended, with probability $0.8$.\n",
    "- To the left of the direction intended with probability $0.1$. \n",
    "- To the right of the direction intended with probability $0.1$. \n",
    "\n",
    "For each state that we could possibly land in, we need to consider both the **immediate reward** and the **value of that state**.\n",
    "\n",
    "Let's go through an example to see how that works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll work backwards from state $(3,5)$. We saw last time that the value of any action from the terminal state is $0$ because there are no future states and no more rewards to be gained., i.e.: \n",
    "$$Q((3,5),a) = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We step backwards to find $Q((2,5), \\text{down})$. We know that the immediate reward for taking this action is $10$, but we **must also consider** that when the agent takes this action, there is a 10% probability it will move **left**, into an obstacle, and a 10% probability that it will move **right**, out of bounds. Both of these transitions carry a **$-5$ penalty**, and so they must be factored into the value of this action. \n",
    "\n",
    "So we know: \n",
    "- There is a **$0.8$ probability of reward $10$** for moving down.\n",
    "- A **$0.1$ probability of reward $-5$** for moving left.\n",
    "- A **$0.1$ probability of reward $-5$** for moving right.\n",
    "\n",
    "We can sum these to find the immediate expected reward: \n",
    "$$\\mathbb{E}[R]=(0.8 \\times 10) + (0.1 \\times -5) + (0.1 \\times -5) = 7$$\n",
    "\n",
    "$$Q((2,5), \\text{down}) = 7 + 0.9 \\cdot 0 \\cdot 0 = 7$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of working with Q-values (action-values), we're going switch to working with **State Values $V(s)$** to keep things a little bit cleaner. \n",
    "\n",
    "Recall from the previous notebook that the **value of the state** is equal to the **value of the best action in that state**, i.e.: \n",
    "$$V(s) = \\max_a Q(s,a)$$\n",
    "\n",
    "Think of the value of a state as being the 'best opportunity' that state offers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now look at $V((1,5))$.\n",
    "$$V((1,5)) := Q((1,5), \\text{down})$$ \n",
    "\n",
    "We also want to consider the **value** of the next state $V(s')$ in our calculations, so we can propagate distant rewards back through our grid.\n",
    "\n",
    "We know: \n",
    "- There is a **$0.8$ probability of moving to a state with value $7$** but  **no immediate reward** by moving down.\n",
    "- A **$0.1$ probability of immediate reward $-5$** by moving left to a state with **no value**.\n",
    "- A **$0.1$ probability of immediate reward $-5$** by moving right to a state with **no value**.\n",
    "\n",
    "Remembering that we need to discount future values using discount factor $\\gamma=0.9$, our expected *value* is: \n",
    "$$(0.8 \\times [0 + (0.9 \\cdot 7)]) + (0.1 \\times (0+ -5)) + (0.1 \\times (0+-5)) = 4.04$$\n",
    "\n",
    "Each time, we're weighting both the **immediate reward and the discounted value of the next state** by the probability that taking a given action reaches that state, and then **summing across all possible states that may result from that action**.\n",
    "\n",
    "We represent this process mathematically as follows: \n",
    "$$V(s)=\\sum_{s'}P(s'|s,a)[R(s,a,s')+ \\gamma \\cdot V(s')]$$\n",
    "\n",
    "Equivalently: \n",
    "$$Q(s,a)=\\sum_{s'}P(s'|s,a)[R(s,a,s')+ \\gamma \\cdot \\max_{a'} Q(s',a')]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's change our $\\text{update}$ function to reflect this new update rule. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update():\n",
    "    gamma = 0.9\n",
    "    for state in Q_values.keys():\n",
    "\n",
    "        if state == terminal_state or grid[state] == '#':\n",
    "            continue\n",
    "\n",
    "        for action in actions:\n",
    "            possible_moves = get_possible_moves(state, action)\n",
    "\n",
    "            # We initialise the expected value at zero.\n",
    "            expected_value = 0\n",
    "\n",
    "            # We now sum over each possible move\n",
    "            for move, prob in zip(possible_moves, probabilities):\n",
    "                if is_valid(move):\n",
    "                    reward = rewards.get(move, 0)\n",
    "                    next_value = max(Q_values[move].values())\n",
    "\n",
    "                # A penalty of 5 and a zero-value for entering invalid states (obstacle/out of bounds)\n",
    "                else: reward = -5; next_value = 0\n",
    "\n",
    "                # We add P(s'|s, a) * [R + gamma * max Q(s', a')] to our expected value\n",
    "                expected_value += prob * (reward + gamma * next_value)\n",
    "\n",
    "            # Finally, we define our Q-value as the sum of the expected values of each possible move\n",
    "            Q_values[state][action] = expected_value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's iterate over our update rule to give our values a chance to converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "for _ in range(100):\n",
    "    update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And have a look at our new Q-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): {'up': -5.0, 'down': -1.42, 'left': -4.55, 'right': -4.55},\n",
       " (0, 1): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " (0, 2): {'up': -4.29, 'down': 1.3, 'left': -4.3, 'right': 1.36},\n",
       " (0, 3): {'up': -3.62, 'down': 2.3, 'left': 0.72, 'right': 1.78},\n",
       " (0, 4): {'up': -3.5, 'down': 2.83, 'left': 1.45, 'right': 2.12},\n",
       " (0, 5): {'up': -4.25, 'down': 3.23, 'left': 1.97, 'right': -4.07},\n",
       " (1, 0): {'up': -2.02, 'down': -0.58, 'left': -4.08, 'right': -4.08},\n",
       " (1, 1): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " (1, 2): {'up': 0.72, 'down': 1.05, 'left': -3.71, 'right': 2.21},\n",
       " (1, 3): {'up': 2.15, 'down': 1.64, 'left': 1.94, 'right': 2.68},\n",
       " (1, 4): {'up': 2.71, 'down': -3.32, 'left': 1.68, 'right': 3.23},\n",
       " (1, 5): {'up': 2.12, 'down': 4.83, 'left': 3.25, 'right': -3.08},\n",
       " (2, 0): {'up': -0.84, 'down': -0.38, 'left': -4.05, 'right': 0.58},\n",
       " (2, 1): {'up': -3.78, 'down': 0.73, 'left': -0.02, 'right': 0.87},\n",
       " (2, 2): {'up': 1.82, 'down': 0.85, 'left': 0.91, 'right': 1.42},\n",
       " (2, 3): {'up': 1.59, 'down': -4.34, 'left': 1.05, 'right': -4.26},\n",
       " (2, 4): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " (2, 5): {'up': 2.48, 'down': 7.0, 'left': -2.57, 'right': -2.57},\n",
       " (3, 0): {'up': -0.02, 'down': -4.44, 'left': -4.45, 'right': 0.06},\n",
       " (3, 1): {'up': 0.71, 'down': -3.92, 'left': -0.38, 'right': 0.21},\n",
       " (3, 2): {'up': 0.87, 'down': -4.44, 'left': 0.18, 'right': -4.34},\n",
       " (3, 3): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " (3, 4): {'up': -3.5, 'down': -3.5, 'left': -5.0, 'right': 7.0},\n",
       " (3, 5): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 0.0}}"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_values = {\n",
    "    state: {action: round(value, 2) for action, value in actions.items()}\n",
    "    for state, actions in Q_values.items()\n",
    "}\n",
    "Q_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now see how our agent navigates our grid. We can see some slipping, but the agent manages to get itself back on the right track."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S # - - ↓ ↓\n",
      "↓ # → → → ↓\n",
      "→ → ↑ - # ↓\n",
      "- - - # - F\n"
     ]
    }
   ],
   "source": [
    "visualise_path()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policies\n",
    "We have previously considered only **deterministically selecting the best action** when calculating our state values and action values: \n",
    "$$V^{\\pi^*}(s)=\\sum_{s'}P(s'|s,a)[R(s,a,s')+ \\gamma \\cdot V^{\\pi^*}(s')]$$\n",
    "\n",
    "Equivalently: \n",
    "$$Q^{\\pi^*}(s,a)=\\sum_{s'}P(s'|s,a)[R(s,a,s')+ \\gamma \\cdot \\max_{a'} Q(s',a')]$$\n",
    "where we introduce new notation $V^{\\pi^*}(s)$ and $Q^{\\pi^*}(s,a)$ to denote the state values and action values **calculated under the assumption of following the optimal policy $\\pi^*$**. \n",
    "\n",
    "Computing the optimal state-values and action-values using these equations is known as **Value Iteration**. \n",
    "\n",
    "Since we have previously defined the optimal policy as: \n",
    "$$\\pi^*(s) = \\arg\\max_{a}Q(s',a')$$\n",
    "\n",
    "We can build a **policy table** similar to our Q-table by extracting the best action for each state $\\arg\\max_{a}Q(s',a')$ from our Q-table, and assigning it a 100% probability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): {'up': 0.0, 'down': 1.0, 'left': 0.0, 'right': 0.0},\n",
       " (0, 1): {'up': 1.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " (0, 2): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 1.0},\n",
       " (0, 3): {'up': 0.0, 'down': 1.0, 'left': 0.0, 'right': 0.0},\n",
       " (0, 4): {'up': 0.0, 'down': 1.0, 'left': 0.0, 'right': 0.0},\n",
       " (0, 5): {'up': 0.0, 'down': 1.0, 'left': 0.0, 'right': 0.0},\n",
       " (1, 0): {'up': 0.0, 'down': 1.0, 'left': 0.0, 'right': 0.0},\n",
       " (1, 1): {'up': 1.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " (1, 2): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 1.0},\n",
       " (1, 3): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 1.0},\n",
       " (1, 4): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 1.0},\n",
       " (1, 5): {'up': 0.0, 'down': 1.0, 'left': 0.0, 'right': 0.0},\n",
       " (2, 0): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 1.0},\n",
       " (2, 1): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 1.0},\n",
       " (2, 2): {'up': 1.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " (2, 3): {'up': 1.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " (2, 4): {'up': 1.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " (2, 5): {'up': 0.0, 'down': 1.0, 'left': 0.0, 'right': 0.0},\n",
       " (3, 0): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 1.0},\n",
       " (3, 1): {'up': 1.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " (3, 2): {'up': 1.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " (3, 3): {'up': 1.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " (3, 4): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 1.0},\n",
       " (3, 5): {'up': 1.0, 'down': 0.0, 'left': 0.0, 'right': 0.0}}"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build a policy table with the same structure as our Q-table\n",
    "optimal_policy = { (i, j): {a: 0.0 for a in actions}\n",
    "                  for i in range(rows) for j in range(cols)}\n",
    "\n",
    "# Iterate over every state in the Q-table\n",
    "for state in Q_values:\n",
    "    \n",
    "    # Extract the best action from the state\n",
    "    best_action = max(Q_values[state], key=Q_values[state].get)\n",
    "\n",
    "    # Iterate over each action in the policy table and assign a \n",
    "    # probability of 1.0 if it had the highest Q-value\n",
    "    for action in actions:\n",
    "        optimal_policy[state][action] = 1.0 if action == best_action else 0.0\n",
    "    \n",
    "optimal_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we have **extracted the optimal policy $\\pi^*(s)$**. Our agent can use this to traverse the grid by selecting the action with $p=1.0$ in each state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Policies\n",
    "\n",
    "Previously, we have seen how the agent navigates the grid using a **deterministic policy** (specifically the optimal policy), where it simply picks the action with the highest Q-value in each state: \n",
    "$$\\pi^*(a|s)=\\arg \\max_{a}Q(s,a)$$\n",
    "\n",
    "We will now consider situations in which the agent will navigate the grid using a **Stochastic Policy**, where the **next action is sampled from a probability distribution across the action space**: \n",
    "$$\\pi(a|s)=P(a|s)$$\n",
    "where the policy for taking action $a$ in state $s$ is given by the probability for taking action $a$ in state $s$.\n",
    "\n",
    "One way to define a stochastic policy is by converting Q-values into probabilities using a softmax distribution (which we introduced in the previous notebook):\n",
    "$$P(a)=\\frac{e^{Q(a)}}{\\sum_{a}e^{Q(a)}}$$\n",
    "\n",
    "We can introduce a new temperature parameter, $\\tau$, to control the shape of the distribution:\n",
    "$$P(a)=\\frac{e^{\\frac{Q(a)}{\\tau}}}{\\sum_{a}e^{\\frac{Q(a)}{\\tau}}}$$\n",
    "- A high $\\tau$ results in a more uniform probability. \n",
    "- A low $\\tau$ results in a more determinstic policy, with a preference towards optimal values. \n",
    "This is useful for controlling **exploration vs. exploitation**. \n",
    "\n",
    "Let's see how that works by looking at the Q-values for state $(2,0)$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'up': -0.84, 'down': -0.38, 'left': -4.05, 'right': 0.58}"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_values[(2,0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's map those to probabilities in the range $[0,1]$ using our softmax function, and experiment with different values of our temperature parameter tau, $\\tau$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tau = 1: {'up': 0.148, 'down': 0.234, 'left': 0.006, 'right': 0.612}\n",
      "Tau = 10: {'up': 0.255, 'down': 0.267, 'left': 0.185, 'right': 0.294}\n",
      "Tau = 0.1: {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 1.0}\n"
     ]
    }
   ],
   "source": [
    "def softmax(q_values, tau=1.0):\n",
    "    values = np.array(list(q_values.values()))\n",
    "    exp = np.exp(values/tau)\n",
    "    probabilities = (exp / np.sum(exp)).tolist()\n",
    "\n",
    "    return ({action: round(prob,3) for action, prob in zip(q_values.keys(), probabilities)})\n",
    "\n",
    "print(f\"Tau = 1: {softmax(Q_values[(2,0)], 1)}\")\n",
    "print(f\"Tau = 10: {softmax(Q_values[(2,0)], 10)}\")\n",
    "print(f\"Tau = 0.1: {softmax(Q_values[(2,0)], 0.1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we apply this technique to the Q-values across our Q-table, we can extract a stochastic policy $\\pi(a|s)$ defined by our Q-values from our Q-table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): {'up': 0.025, 'down': 0.897, 'left': 0.039, 'right': 0.039},\n",
       " (0, 1): {'up': 0.25, 'down': 0.25, 'left': 0.25, 'right': 0.25},\n",
       " (0, 2): {'up': 0.002, 'down': 0.483, 'left': 0.002, 'right': 0.513},\n",
       " (0, 3): {'up': 0.001, 'down': 0.555, 'left': 0.114, 'right': 0.33},\n",
       " (0, 4): {'up': 0.001, 'down': 0.573, 'left': 0.144, 'right': 0.282},\n",
       " (0, 5): {'up': 0.0, 'down': 0.778, 'left': 0.221, 'right': 0.001},\n",
       " (1, 0): {'up': 0.183, 'down': 0.771, 'left': 0.023, 'right': 0.023},\n",
       " (1, 1): {'up': 0.25, 'down': 0.25, 'left': 0.25, 'right': 0.25},\n",
       " (1, 2): {'up': 0.146, 'down': 0.203, 'left': 0.002, 'right': 0.649},\n",
       " (1, 3): {'up': 0.243, 'down': 0.146, 'left': 0.197, 'right': 0.413},\n",
       " (1, 4): {'up': 0.329, 'down': 0.001, 'left': 0.117, 'right': 0.553},\n",
       " (1, 5): {'up': 0.052, 'down': 0.786, 'left': 0.162, 'right': 0.0},\n",
       " (2, 0): {'up': 0.148, 'down': 0.234, 'left': 0.006, 'right': 0.612},\n",
       " (2, 1): {'up': 0.004, 'down': 0.38, 'left': 0.179, 'right': 0.437},\n",
       " (2, 2): {'up': 0.408, 'down': 0.155, 'left': 0.164, 'right': 0.273},\n",
       " (2, 3): {'up': 0.63, 'down': 0.002, 'left': 0.367, 'right': 0.002},\n",
       " (2, 4): {'up': 0.25, 'down': 0.25, 'left': 0.25, 'right': 0.25},\n",
       " (2, 5): {'up': 0.011, 'down': 0.989, 'left': 0.0, 'right': 0.0},\n",
       " (3, 0): {'up': 0.475, 'down': 0.006, 'left': 0.006, 'right': 0.514},\n",
       " (3, 1): {'up': 0.512, 'down': 0.005, 'left': 0.172, 'right': 0.311},\n",
       " (3, 2): {'up': 0.661, 'down': 0.003, 'left': 0.332, 'right': 0.004},\n",
       " (3, 3): {'up': 0.25, 'down': 0.25, 'left': 0.25, 'right': 0.25},\n",
       " (3, 4): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 1.0},\n",
       " (3, 5): {'up': 0.25, 'down': 0.25, 'left': 0.25, 'right': 0.25}}"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy = { (i, j): {a: 0.0 for a in actions}\n",
    "                  for i in range(rows) for j in range(cols)}\n",
    "\n",
    "tau = 1.0\n",
    "\n",
    "# Iterate over each state\n",
    "for state in policy:\n",
    "\n",
    "    # Softmax our Q-values to calculate action probabilities\n",
    "    values = np.array(list(Q_values[state].values()))\n",
    "    exp = np.exp(values/tau)\n",
    "    probabilities = (exp / np.sum(exp)).tolist()\n",
    "\n",
    "    # Update our action probabilities \n",
    "    policy[state] = {a: round(prob, 3) for a, prob in zip(actions, probabilities)}\n",
    "    \n",
    "policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bellman Equations\n",
    "Because we're not deterministically selecting the same trajectory every time we navigate the grid, the expected return for each state-action pair is going to change, which needs to be reflected in our state values $V(s)$ and action values $Q(s,a)$. \n",
    "\n",
    "In similar fashion to accounting for our stochastic state transitions, we **need to weigh the probability of each action in each state** when determining the value. \n",
    "\n",
    "Instead of following the **optimal policy update rule**: \n",
    "$$Q^{\\pi^*}(s,a)=\\sum_{s'}P(s'|s,a)[R(s,a,s')+ \\gamma \\cdot \\max_{a'} Q(s',a')]$$\n",
    "\n",
    "Under stochastic policy $\\pi$, we'd want to set the value of the action $Q(s,a)$ equal to:\n",
    "$$Q^\\pi(s,a)=\\sum_{s'}P(s'|s,a)[R(s,a,s')+ \\gamma \\cdot \\text{???}]$$\n",
    "\n",
    "Instead of considering only $\\max_{a'}Q(s',a')$ (as we did when following the *optimal* policy), we must now factor in **the probability of each action being selected**, i.e., we must consider $\\pi(a'|s')$, and weigh the value of $Q^\\pi(s',a')$ accordingly. \n",
    "\n",
    "To do this, we simply **multiply the action value of each action by the probability of it happening and sum them together**:\n",
    "$$Q^\\pi(s,a)=\\sum_{s'} P(s'|s,a)[R(s,a,s') + \\gamma \\cdot \\sum_{a'} \\pi(a'|s') \\cdot Q^\\pi(s',a')]$$ \n",
    "\n",
    "This is the **Bellman expectation equation** for the action-value function $Q^\\pi(s,a)$, which defines the expected return when starting in state $s$, taking action $a$, and then following policy $\\pi$ thereafter. Instead of selecting the best action as in the optimal case $\\max_{a'}Q(s',a')$, we take the expected value over all actions weighted by their probabilities under the policy $\\pi(a'|s')$. \n",
    "\n",
    "Similarly, we can also define the Bellman expectation equation for the state-value function $V^\\pi(s)$: \n",
    "$$V^\\pi(s)=\\sum_a\\pi(a|s)\\sum_{s'}P(s'|s,a)[R(s,a,s')+\\gamma \\cdot V^\\pi(s')]$$\n",
    "\n",
    "This equation computes the expected value of a state under policy $\\pi$ by averaging over all possible actions and state transitions.\n",
    "\n",
    "Since $V^\\pi(s)$ represents the expected return when following $\\pi$, it is an average over actions weighted by $\\pi(a|s)$, rather than selecting the best action. As a result, we do not define the value of a state as the value of the best action, meaning $V^\\pi(s) \\neq \\max_a Q(s', a')$. \n",
    "\n",
    "Computing the value function $V^\\pi(s)$ or the action-value function $Q^\\pi(s,a)$ for a given policy $\\pi$ is known as **Policy Evaluation**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Policy Iteration\n",
    "So far, we have seen how we can use **Value Iteration** to compute state- and action-values, which we then use to extract the optimal policy $\\pi^*(s)$ or a stochastic policy $\\pi(a|s)$. \n",
    "\n",
    "Instead of deriving a policy after computing Q-values, we can now construct the optimal policy from scratch using **Policy Iteration**. \n",
    "\n",
    "Policy iteration consists of two alternating steps:\n",
    "1. **Policy Evaluation**: Compute $V^\\pi(s)$ or $Q^\\pi(s,a)$ for the current policy $\\pi$ using the equations derived above. \n",
    "2. **Policy Improvement**: Update $\\pi(s)$ by selecting actions that lead to higher returns. \n",
    "\n",
    "By iterating over these two steps, we can refine our policy until it converges to an optimal policy $\\pi^*$. \n",
    "\n",
    "##### Policy Evaluation \n",
    "$$Q^\\pi(s,a)=\\sum_{s'} P(s'|s,a)[R(s,a,s') + \\gamma \\cdot \\sum_{a'} \\pi(a'|s') \\cdot Q^\\pi(s',a')]$$ \n",
    "\n",
    "$$V^\\pi(s)=\\sum_a\\pi(a|s)\\sum_{s'}P(s'|s,a)[R(s,a,s')+\\gamma \\cdot V^\\pi(s')]$$\n",
    "\n",
    "##### Policy Improvement\n",
    "\n",
    "$$\\pi'(s)=\\arg\\max_a\\sum_{s'}P(s'|s,a)[R(s,a,s')+\\gamma\\cdot V^\\pi(s')]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "policy = { (i, j): {a: 0.25 for a in actions} for i in range(rows) for j in range(cols)}\n",
    "state_values = { (i, j): 0.0 for i in range(rows) for j in range(cols )}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): {'up': 0.25, 'down': 0.25, 'left': 0.25, 'right': 0.25},\n",
       " (0, 1): {'up': 0.25, 'down': 0.25, 'left': 0.25, 'right': 0.25},\n",
       " (0, 2): {'up': 0.25, 'down': 0.25, 'left': 0.25, 'right': 0.25},\n",
       " (0, 3): {'up': 0.25, 'down': 0.25, 'left': 0.25, 'right': 0.25},\n",
       " (0, 4): {'up': 0.25, 'down': 0.25, 'left': 0.25, 'right': 0.25},\n",
       " (0, 5): {'up': 0.25, 'down': 0.25, 'left': 0.25, 'right': 0.25},\n",
       " (1, 0): {'up': 0.25, 'down': 0.25, 'left': 0.25, 'right': 0.25},\n",
       " (1, 1): {'up': 0.25, 'down': 0.25, 'left': 0.25, 'right': 0.25},\n",
       " (1, 2): {'up': 0.25, 'down': 0.25, 'left': 0.25, 'right': 0.25},\n",
       " (1, 3): {'up': 0.25, 'down': 0.25, 'left': 0.25, 'right': 0.25},\n",
       " (1, 4): {'up': 0.25, 'down': 0.25, 'left': 0.25, 'right': 0.25},\n",
       " (1, 5): {'up': 0.25, 'down': 0.25, 'left': 0.25, 'right': 0.25},\n",
       " (2, 0): {'up': 0.25, 'down': 0.25, 'left': 0.25, 'right': 0.25},\n",
       " (2, 1): {'up': 0.25, 'down': 0.25, 'left': 0.25, 'right': 0.25},\n",
       " (2, 2): {'up': 0.25, 'down': 0.25, 'left': 0.25, 'right': 0.25},\n",
       " (2, 3): {'up': 0.25, 'down': 0.25, 'left': 0.25, 'right': 0.25},\n",
       " (2, 4): {'up': 0.25, 'down': 0.25, 'left': 0.25, 'right': 0.25},\n",
       " (2, 5): {'up': 0.25, 'down': 0.25, 'left': 0.25, 'right': 0.25},\n",
       " (3, 0): {'up': 0.25, 'down': 0.25, 'left': 0.25, 'right': 0.25},\n",
       " (3, 1): {'up': 0.25, 'down': 0.25, 'left': 0.25, 'right': 0.25},\n",
       " (3, 2): {'up': 0.25, 'down': 0.25, 'left': 0.25, 'right': 0.25},\n",
       " (3, 3): {'up': 0.25, 'down': 0.25, 'left': 0.25, 'right': 0.25},\n",
       " (3, 4): {'up': 0.25, 'down': 0.25, 'left': 0.25, 'right': 0.25},\n",
       " (3, 5): {'up': 0.25, 'down': 0.25, 'left': 0.25, 'right': 0.25}}"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): 0.0,\n",
       " (0, 1): 0.0,\n",
       " (0, 2): 0.0,\n",
       " (0, 3): 0.0,\n",
       " (0, 4): 0.0,\n",
       " (0, 5): 0.0,\n",
       " (1, 0): 0.0,\n",
       " (1, 1): 0.0,\n",
       " (1, 2): 0.0,\n",
       " (1, 3): 0.0,\n",
       " (1, 4): 0.0,\n",
       " (1, 5): 0.0,\n",
       " (2, 0): 0.0,\n",
       " (2, 1): 0.0,\n",
       " (2, 2): 0.0,\n",
       " (2, 3): 0.0,\n",
       " (2, 4): 0.0,\n",
       " (2, 5): 0.0,\n",
       " (3, 0): 0.0,\n",
       " (3, 1): 0.0,\n",
       " (3, 2): 0.0,\n",
       " (3, 3): 0.0,\n",
       " (3, 4): 0.0,\n",
       " (3, 5): 0.0}"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = [0.8, 0.1, 0.1]\n",
    "\n",
    "actions = ['up', 'down', 'left', 'right']\n",
    "\n",
    "def get_possible_moves(state, action):\n",
    "    i, j = state\n",
    "    if action == 'up':\n",
    "        return [(i-1, j), (i, j-1), (i, j+1)] # up, left, right\n",
    "    elif action == 'down':\n",
    "        return [(i+1, j), (i, j-1), (i, j+1)] # down, left, right\n",
    "    elif action == 'left':\n",
    "        return [(i, j-1), (i-1, j), (i+1, j)] # left, up, down\n",
    "    elif action == 'right':\n",
    "        return [(i, j+1), (i-1, j), (i+1, j)] # right, up, down "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Policy Evaluation\n",
    "\n",
    "$$V^\\pi(s)=\\sum_a\\pi(a|s)\\sum_{s'}P(s'|s,a)[R(s,a,s')+\\gamma \\cdot V^\\pi(s')]$$\n",
    "\n",
    "We want to do three things:\n",
    "1. Calculate the value of taking an action in a given state, summing over each outcome weighted by the probability of it happening.\n",
    "2. Multiply this value by the probability of it being selected under the current policy.\n",
    "3. Sum over all possible actions in a given state.\n",
    "\n",
    "Policy evaluation will update the **state values** only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation():\n",
    "    # We want to update every possible state\n",
    "    for state in state_values:\n",
    "        # Ignore terminal states and obstacles (they don't have a value)\n",
    "        if state == terminal_state or grid[state] == '#':\n",
    "            continue\n",
    "\n",
    "        v = 0\n",
    "        # Σ π(a|s): Sum over all actions weighted by their probability\n",
    "        for action in actions:\n",
    "            # π(a|s): The probability of taking this action under the current policy\n",
    "            action_probability = policy[state][action]\n",
    "            \n",
    "            # Get the set of possible moves by taking this action\n",
    "            # Remember we can go intended direction or perpendicular\n",
    "            possible_moves = get_possible_moves(state, action)\n",
    "\n",
    "            # Set a placeholder for the value of the action\n",
    "            expected_value = 0\n",
    "\n",
    "            # Σ P(s'|s,a): Sum over each state and weigh by its probability\n",
    "            for move, prob in zip(possible_moves, probabilities):\n",
    "                if is_valid(move):\n",
    "                    reward = rewards.get(move, 0)\n",
    "\n",
    "                    # P(s'|s,a) * [R + γ * V(s)]\n",
    "                    expected_value += prob * (reward + gamma * state_values[move])\n",
    "                \n",
    "                else: expected_value += prob * (-5)\n",
    "            \n",
    "            # π(a|s) * Σ P(s'|s,a) * [R + γ * V(s)]\n",
    "            # Update the value of the state\n",
    "            v += action_probability * expected_value\n",
    "        \n",
    "        # Update V(s)\n",
    "        state_values[state] = v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Policy Improvement\n",
    "\n",
    "$$\\pi'(s)=\\arg\\max_a\\sum_{s'}P(s'|s,a)[R(s,a,s')+\\gamma\\cdot V^\\pi(s')]$$\n",
    "\n",
    "We want to modify the policy to select the action that leads to the highest next-state value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement():\n",
    "    for state in state_values:\n",
    "\n",
    "        # Initialise a dictionary to store the value of each action\n",
    "        action_values = {}\n",
    "\n",
    "        for action in actions:\n",
    "            possible_moves = get_possible_moves(state, action)\n",
    "            expected_value = 0\n",
    "\n",
    "            for move, prob in zip(possible_moves, probabilities):\n",
    "                if is_valid(move):\n",
    "                    reward = rewards.get(move, 0)\n",
    "                    expected_value += prob * (reward * gamma * state_values[move])\n",
    "                else:\n",
    "                    expected_value += prob * (-5)\n",
    "            \n",
    "            action_values[action] = expected_value\n",
    "        \n",
    "        # arg max_a: Select the best action from the state's action set\n",
    "        best_action = max(action_values, key=action_values.get)\n",
    "\n",
    "        for action in actions:\n",
    "            policy[state][action] = 1.0 if action == best_action else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(iterations: int):\n",
    "    for i in range(iterations):\n",
    "        policy_evaluation()\n",
    "        policy_improvement()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): -3.75,\n",
       " (0, 1): 0.0,\n",
       " (0, 2): -2.5,\n",
       " (0, 3): -1.8124999999999998,\n",
       " (0, 4): -1.6578125,\n",
       " (0, 5): -2.8730078125,\n",
       " (1, 0): -3.3437500000000004,\n",
       " (1, 1): 0.0,\n",
       " (1, 2): -1.8124999999999998,\n",
       " (1, 3): -0.8156249999999999,\n",
       " (1, 4): -1.8065234374999997,\n",
       " (1, 5): -2.3028945312499998,\n",
       " (2, 0): -2.00234375,\n",
       " (2, 1): -1.7005273437500001,\n",
       " (2, 2): -0.79043115234375,\n",
       " (2, 3): -2.8613626342773437,\n",
       " (2, 4): 0.0,\n",
       " (2, 5): -0.5181512695312502,\n",
       " (3, 0): -2.95052734375,\n",
       " (3, 1): -2.2964873046875005,\n",
       " (3, 2): -3.1945566528320315,\n",
       " (3, 3): 0.0,\n",
       " (3, 4): -1.25,\n",
       " (3, 5): 0.0}"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_improvement()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): {'up': 0.0, 'down': 1.0, 'left': 0.0, 'right': 0.0},\n",
       " (0, 1): {'up': 0.0, 'down': 0.0, 'left': 1.0, 'right': 0.0},\n",
       " (0, 2): {'up': 0.0, 'down': 1.0, 'left': 0.0, 'right': 0.0},\n",
       " (0, 3): {'up': 0.0, 'down': 1.0, 'left': 0.0, 'right': 0.0},\n",
       " (0, 4): {'up': 0.0, 'down': 1.0, 'left': 0.0, 'right': 0.0},\n",
       " (0, 5): {'up': 0.0, 'down': 1.0, 'left': 0.0, 'right': 0.0},\n",
       " (1, 0): {'up': 1.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " (1, 1): {'up': 0.0, 'down': 1.0, 'left': 0.0, 'right': 0.0},\n",
       " (1, 2): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 1.0},\n",
       " (1, 3): {'up': 1.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " (1, 4): {'up': 1.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " (1, 5): {'up': 0.0, 'down': 0.0, 'left': 1.0, 'right': 0.0},\n",
       " (2, 0): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 1.0},\n",
       " (2, 1): {'up': 0.0, 'down': 1.0, 'left': 0.0, 'right': 0.0},\n",
       " (2, 2): {'up': 1.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " (2, 3): {'up': 1.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " (2, 4): {'up': 1.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " (2, 5): {'up': 1.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " (3, 0): {'up': 1.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " (3, 1): {'up': 1.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " (3, 2): {'up': 1.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " (3, 3): {'up': 1.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " (3, 4): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 1.0},\n",
       " (3, 5): {'up': 1.0, 'down': 0.0, 'left': 0.0, 'right': 0.0}}"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S # - - ↓ ↓\n",
      "↓ # → → → ↓\n",
      "→ → ↑ - # ↓\n",
      "- - - # - F\n"
     ]
    }
   ],
   "source": [
    "visualise_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S # - - - -\n",
      "↓ # → → → ↓\n",
      "→ → ↑ - # ↓\n",
      "- ↑ - # - F\n"
     ]
    }
   ],
   "source": [
    "policy_iteration(10)\n",
    "visualise_path()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
