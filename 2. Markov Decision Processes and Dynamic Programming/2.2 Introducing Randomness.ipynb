{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Markov Decision Processes\n",
    "## 2.2 Introducing Randomness\n",
    "\n",
    "- Random transition probabilities $P(s'|s,a)$\n",
    "- Non-deterministic policies $\\pi(a|s)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deterministic Transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S # - - - -\n",
      "↓ # → → → ↓\n",
      "→ → ↑ - # ↓\n",
      "- - - # - F\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "gamma = 0.9\n",
    "\n",
    "grid = np.array([\n",
    "    ['S', '#', '-', '-', '-', '-'],\n",
    "    ['-', '#', '-', '-', '-', '-'],\n",
    "    ['-', '-', '-', '-', '#', '-'],\n",
    "    ['-', '-', '-', '#', '-', 'F']\n",
    "])\n",
    "\n",
    "rows, cols = grid.shape\n",
    "terminal_state = (3, 5)\n",
    "\n",
    "actions = ['up', 'down', 'left', 'right']\n",
    "\n",
    "rewards = { terminal_state: 10}\n",
    "\n",
    "Q_values = { (i, j): {a: 0.0 for a in actions} for i in range(rows) for j in range(cols) }\n",
    "\n",
    "def is_valid(state):\n",
    "    i, j = state\n",
    "    return 0 <= i < rows and 0 <= j < cols and grid[i, j] != '#'\n",
    "\n",
    "def get_next_state(state, action):\n",
    "    i, j = state\n",
    "    if action == 'up': next_state = (i-1, j)\n",
    "    elif action == 'down': next_state = (i+1, j)\n",
    "    elif action == 'left': next_state = (i, j-1)\n",
    "    elif action == 'right': next_state = (i, j+1)\n",
    "    \n",
    "    if is_valid(next_state): return True, next_state\n",
    "    else: return False, state\n",
    "\n",
    "def update():\n",
    "    for state in Q_values.keys():\n",
    "        if state == terminal_state or grid[state] == '#':\n",
    "            continue\n",
    "        for action in actions:\n",
    "            is_valid, next_state = get_next_state(state, action) \n",
    "            if is_valid:\n",
    "                reward = rewards.get(next_state, 0) \n",
    "                Q_values[state][action] = reward + gamma * max(Q_values[next_state].values())\n",
    "            else: \n",
    "                Q_values[state][action] = -5\n",
    "\n",
    "action_symbols = {\n",
    "    \"up\": \"↑\",\n",
    "    \"down\": \"↓\",\n",
    "    \"left\": \"←\",\n",
    "    \"right\": \"→\"\n",
    "}\n",
    "\n",
    "def get_best_action(state):\n",
    "    return max(Q_values[state], key=Q_values[state].get)\n",
    "\n",
    "def visualise_path():\n",
    "    grid_viz = grid.copy()\n",
    "\n",
    "    state = (0, 0)\n",
    "    path = []\n",
    "    max_steps = 1000\n",
    "\n",
    "    for _ in range(max_steps):\n",
    "        path.append(state)\n",
    "        if state == terminal_state:\n",
    "            break\n",
    "\n",
    "        best_action = get_best_action(state)\n",
    "        _, next_state = get_next_state(state, best_action)\n",
    "\n",
    "        if next_state == state:\n",
    "            pass\n",
    "\n",
    "        state = next_state\n",
    "    \n",
    "    for (i, j) in path:\n",
    "        if grid_viz[i, j] not in ['S', 'F']:\n",
    "            grid_viz[i, j] = action_symbols[get_best_action((i, j))]\n",
    "\n",
    "    for row in grid_viz:\n",
    "        print(\" \".join(row))\n",
    "        \n",
    "for i in range(50):\n",
    "    update()\n",
    "\n",
    "visualise_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): {'up': -5, 'down': 3.87, 'left': -5, 'right': -5},\n",
       " (0, 1): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " (0, 2): {'up': -5, 'down': 5.9, 'left': -5, 'right': 5.9},\n",
       " (0, 3): {'up': -5, 'down': 6.56, 'left': 5.31, 'right': 6.56},\n",
       " (0, 4): {'up': -5, 'down': 7.29, 'left': 5.9, 'right': 7.29},\n",
       " (0, 5): {'up': -5, 'down': 8.1, 'left': 6.56, 'right': -5},\n",
       " (1, 0): {'up': 3.49, 'down': 4.3, 'left': -5, 'right': -5},\n",
       " (1, 1): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " (1, 2): {'up': 5.31, 'down': 5.31, 'left': -5, 'right': 6.56},\n",
       " (1, 3): {'up': 5.9, 'down': 5.9, 'left': 5.9, 'right': 7.29},\n",
       " (1, 4): {'up': 6.56, 'down': -5, 'left': 6.56, 'right': 8.1},\n",
       " (1, 5): {'up': 7.29, 'down': 9.0, 'left': 7.29, 'right': -5},\n",
       " (2, 0): {'up': 3.87, 'down': 3.87, 'left': -5, 'right': 4.78},\n",
       " (2, 1): {'up': -5, 'down': 4.3, 'left': 4.3, 'right': 5.31},\n",
       " (2, 2): {'up': 5.9, 'down': 4.78, 'left': 4.78, 'right': 5.9},\n",
       " (2, 3): {'up': 6.56, 'down': -5, 'left': 5.31, 'right': -5},\n",
       " (2, 4): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " (2, 5): {'up': 8.1, 'down': 10.0, 'left': -5, 'right': -5},\n",
       " (3, 0): {'up': 4.3, 'down': -5, 'left': -5, 'right': 4.3},\n",
       " (3, 1): {'up': 4.78, 'down': -5, 'left': 3.87, 'right': 4.78},\n",
       " (3, 2): {'up': 5.31, 'down': -5, 'left': 4.3, 'right': -5},\n",
       " (3, 3): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " (3, 4): {'up': -5, 'down': -5, 'left': -5, 'right': 10.0},\n",
       " (3, 5): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 0.0}}"
      ]
     },
     "execution_count": 1101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_values = {\n",
    "    state: {action: round(value, 2) for action, value in actions.items()}\n",
    "    for state, actions in Q_values.items()\n",
    "}\n",
    "Q_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Deterministic Transitions\n",
    "Recall the formulation of a Markov decision process:\n",
    "$$(\\mathcal{S}, \\mathcal{A}, P, \\mathcal{R}, \\gamma)$$\n",
    "\n",
    "Where: \n",
    "- $\\mathcal{S}$ is the **state space**.\n",
    "- $\\mathcal{A}$ is the **action space**.\n",
    "- **$P(s'|s,a)$ is the **transition function** - the probability of reaching state $s'$ by taking action $a$ in state $s$.** \n",
    "- $R(s,a,s')$ is the reward received immediately after taking action $a$ in state $s$. \n",
    "- $\\gamma$ is a discount factor.\n",
    "\n",
    "Previously, in our gridworld, we worked by the assumption that state transitions were **completely deterministic**, i.e.:\n",
    "$$P(s'|s,a) = 1 \\quad \\forall s,a \\in \\mathcal{S}, \\mathcal{A}$$\n",
    "\n",
    "This means that every time we took a given action in a given state, we would move to the next state with 100% certainty every single time. For example, if we took action '$\\text{right}$' in state $(0,0)$, our agent will *always* move to state $(0,1)$. \n",
    "\n",
    "In the real world, state transitions are usually not determinstic, but instead are **probabilistic (stochastic)**. \n",
    "\n",
    "Let's go back to our gridworld. This time, the grid is slippery - instead of moving in our intended direction with 100% certainty, there is now a chance of slipping and moving in an unintended direction. \n",
    "\n",
    "Instead, when an action is chosen, we will: \n",
    "- Move in the intended direction with $p=0.8$.\n",
    "- Move left of the intended direction with $p=0.1$.\n",
    "- Move right of the intended direction with $p=0.1$.\n",
    "- Remain in the same state if the action results in an invalid move.\n",
    "\n",
    "Now our **state transition function $P(s'|s,a)$** becomes: \n",
    "$$P(s'|s,a)=\n",
    "\\begin{cases}\n",
    "0.8, & \\text{if } s' \\text{ is the intended direction from } s \\text{ given } a \\\\\n",
    "0.1, & \\text{if } s' \\text{ is to the left of the the intended direction from } s  \\\\\n",
    "0.1, & \\text{if } s' \\text{ is to the right of the intended direction from } s  \\\\\n",
    "0, & \\text{otherwise}\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid(state):\n",
    "    i, j = state\n",
    "    return 0 <= i < rows and 0 <= j < cols and grid[i, j] != '#'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update():\n",
    "    gamma = 0.9\n",
    "    for state in Q_values.keys():\n",
    "\n",
    "        if state == terminal_state or grid[state] == '#':\n",
    "            continue\n",
    "\n",
    "        for action in actions:\n",
    "            is_valid, next_state = get_next_state(state, action)\n",
    "            if is_valid:\n",
    "                reward = rewards.get(next_state, 0)\n",
    "                Q_values[state][action] = reward + gamma * max(Q_values[next_state].values())\n",
    "            else: \n",
    "                Q_values[state][action] = -5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To introduce this stochasticity into our gridworld, we introduce a new function: $\\text{get\\_possible\\_moves}$. This function will return a list of three tuples, with the $0$-th tuple of the list representing the intended action, and the other two tuples representing a perpendicular move to the intended direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_possible_moves(state, action):\n",
    "    i, j = state\n",
    "    if action == 'up':\n",
    "        return [(i-1, j), (i, j-1), (i, j+1)] # up, left, right\n",
    "    elif action == 'down':\n",
    "        return [(i+1, j), (i, j-1), (i, j+1)] # down, left, right\n",
    "    elif action == 'left':\n",
    "        return [(i, j-1), (i-1, j), (i+1, j)] # left, up, down\n",
    "    elif action == 'right':\n",
    "        return [(i, j+1), (i-1, j), (i+1, j)] # right, up, down "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then update our $\\text{get\\_next\\_state}$ function to first sample the actual move from our set of possible moves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1105,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "probabilities = [0.8, 0.1, 0.1] # p=0.8 of moving in the intended direction, (1-p)/2 for moving perpendicular\n",
    "\n",
    "def get_next_state(state, action):\n",
    "    possible_moves = get_possible_moves(state, action)\n",
    "\n",
    "    next_state = possible_moves[np.random.choice(3, p=probabilities)]\n",
    "\n",
    "    if is_valid(next_state):\n",
    "        return True, next_state\n",
    "    else: \n",
    "        return False, state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform our updates as before, using the equation: \n",
    "$$Q(s,a)=R + \\max_{a'}Q(s',a')$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, we just perform one update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1106,
   "metadata": {},
   "outputs": [],
   "source": [
    "update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can how visualise how an agent following the *optimal policy* $\\pi^*$ (that just means selecting the **best** action in each state every time) navigates the grid after just one update.\n",
    "\n",
    "Recall: \n",
    "$$\\pi^*= \\arg\\max_{a}Q(s,a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this equation to plot our agent's course across the grid. Look closely. We see that in state $(2,3)$ the agent has learned the optimal action '$\\text{left}$'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S # - - - -\n",
      "↓ # - → → ↓\n",
      "→ → → ← # ↓\n",
      "- - - # - F\n"
     ]
    }
   ],
   "source": [
    "visualise_path()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from the previous notebook that the update function iterates over all states and **retrieves the value of the next state corresponding to each action**. It will then use the value of this next state to update its belief about the value of its *own* state. \n",
    "\n",
    "Specifically, we look at this part of the $\\text{update}$ function: \n",
    "```python\n",
    "for action in actions:\n",
    "            is_valid, next_state = get_next_state(state, action)\n",
    "            reward = rewards.get(next_state, 0)\n",
    "                Q_values[state][action] = reward + gamma * max(Q_values[next_state].values())\n",
    "         \n",
    "```\n",
    "Recall how we laid out our state transition probabilities earlier: we said that with probability $0.8$, the action would transition state as intended, but with probability $0.2$, the action would transition state perpendicular to the intended. \n",
    "\n",
    "This means that during our update, the agent tested action '$\\text{left}$' in state $(2,3)$ and, with a probability of 10%, it moved upwards instead (i.e., ended up in state $(1,3)$). It then used the *value of this state* to update its belief about the value of $Q((2,3), \\text{right})$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do another iteration of our update rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1108,
   "metadata": {},
   "outputs": [],
   "source": [
    "update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And visualise the path that it has learned now. Already, it is failing to learn an optimal path to reach the terminal state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S # - ↓ ↓ ↓\n",
      "↓ # → → → ↑\n",
      "→ ↑ ↑ - # -\n",
      "- - - # - F\n"
     ]
    }
   ],
   "source": [
    "visualise_path()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the agent gets stuck in state $(1,5)$. Let's look at the Q-values for this state and see what the agent has learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'up': 7.29, 'down': 7.29, 'left': 7.29, 'right': -5}"
      ]
     },
     "execution_count": 1110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_values[(1,5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Q-values for actions '$\\text{up}$', '$\\text{down}$', and '$\\text{left}$' have been assigned equal value. With each value equal, the agent will simply pick the **first occurrence** of the maximum value, and so it selects action '$\\text{up}$'.\n",
    "\n",
    "We can also visualise what happens when we update our Q-values a few more times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S # - - - -\n",
      "↑ # - - - -\n",
      "- - - - # -\n",
      "- - - # - F\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    update()\n",
    "visualise_path()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent gets stuck almost immediately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To address this, let's consider our update rule: \n",
    "$$Q(s,a)=R + \\gamma_ \\cdot \\max_{a'} Q(s',a')$$\n",
    "\n",
    "When we update our Q-value using this rule, we assume that the **the action always transitions to the best next state**. This means that the agent is not learning the **true expected value of the next state**. \n",
    "\n",
    "Consider an action which has a 90% chance of a reward of $10$, but a 10% chance of a penalty of $100$. Can we assign a value of $10$ to the action just because it is the most probable outcome? No - **we must weight all outcomes** by their chances of happening to get an accurate representation of their value: \n",
    "$$(0.9 * 10) + (0.1 * -100) = -1$$\n",
    "\n",
    "This is **exactly** what we need to consider when we're navigating our grid. Each action has a probability of transitioning to three separate states: \n",
    "- The one intended, with probability $0.8$.\n",
    "- To the left of the direction intended with probability $0.1$. \n",
    "- To the right of the direction intended with probability $0.1$. \n",
    "\n",
    "For each state that we could possibly land in, we need to consider both the **immediate reward** and the **value of that state**.\n",
    "\n",
    "Let's go through an example to see how that works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll work backwards from state $(3,5)$. We saw last time that the value of any action from the terminal state is $0$ because there are no future states and no more rewards to be gained., i.e.: \n",
    "$$Q((3,5),a) = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We step backwards to find $Q((2,5), \\text{down})$. We know that the immediate reward for taking this action is $10$, but we **must also consider** that when the agent takes this action, there is a 10% probability it will move **left**, into an obstacle, and a 10% probability that it will move **right**, out of bounds. Both of these transitions carry a **$-5$ penalty**, and so they must be factored into the value of this action. \n",
    "\n",
    "So we know: \n",
    "- There is a **$0.8$ probability of reward $10$** by moving down.\n",
    "- A **$0.1$ probability of reward $-5$** by moving left.\n",
    "- A **$0.1$ probability of reward $-5$** by moving right.\n",
    "\n",
    "We can sum these to find the immediate expected reward: \n",
    "$$(0.8 \\times 10) + (0.1 \\times -5) + (0.1 \\times -5) = 7$$\n",
    "\n",
    "$$Q((2,5), \\text{down}) = 7 + 0.9 \\cdot 0 \\cdot 0 = 7$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of working with Q-values (action-values), we're going switch to working with **State Values $V(s)$** to keep things a little bit cleaner. \n",
    "\n",
    "Recall from the previous notebook that the **value of the state** is equal to the **value of the best action in that state**, i.e.: \n",
    "$$V(s) = \\max_a Q(s,a)$$\n",
    "\n",
    "Think of the value of a state as being the 'best opportunity' that state offers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now look at $V((1,5))$.\n",
    "$$V((1,5)) := Q((1,5), \\text{down})$$ \n",
    "\n",
    "We also want to consider the **value** of the next state $V(s')$ in our calculations, so we can propagate distant rewards back through our grid.\n",
    "\n",
    "We know: \n",
    "- There is a **$0.8$ probability of moving to a state with value $7$** but  **no immediate reward** by moving down.\n",
    "- A **$0.1$ probability of immediate reward $-5$** by moving left to a state with **no value**.\n",
    "- A **$0.1$ probability of immediate reward $-5$** by moving right to a state with **no value**.\n",
    "\n",
    "Remembering that we need to discount future values using discount factor $\\gamma=0.9$, our expected *value* is: \n",
    "$$(0.8 \\times [0 + (0.9 \\cdot 7)]) + (0.1 \\times (0+ -5)) + (0.1 \\times (0+-5)) = 4.04$$\n",
    "\n",
    "Each time, we're weighting both the **immediate reward and the value of the next state** by the probability that taking a given action reaches that state, and then summing across all possible states that may result from that action.\n",
    "\n",
    "We represent this process mathematically as follows: \n",
    "$$V(s)=\\sum_{s'}P(s'|s,a)[R(s,a,s')+ \\gamma \\cdot V(s')]$$\n",
    "\n",
    "Equivalently: \n",
    "$$Q(s,a)=\\sum_{s'}P(s'|s,a)[R(s,a,s')+ \\gamma \\cdot \\max_{a'} Q(s',a')]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's change our $\\text{update}$ function to reflect this new update rule. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update():\n",
    "    gamma = 0.9\n",
    "    for state in Q_values.keys():\n",
    "\n",
    "        if state == terminal_state or grid[state] == '#':\n",
    "            continue\n",
    "\n",
    "        for action in actions:\n",
    "            possible_moves = get_possible_moves(state, action)\n",
    "            expected_value = 0\n",
    "\n",
    "            # Sum over each possible move\n",
    "            for move, prob in zip(possible_moves, probabilities):\n",
    "                if is_valid(move):\n",
    "                    reward = rewards.get(move, 0)\n",
    "                    next_value = max(Q_values[move].values())\n",
    "                else: reward = -5; next_value = 0\n",
    "                # P(s'|s, a) * [R + gamma * max Q(s', a')]\n",
    "                expected_value += prob * (reward + gamma * next_value)\n",
    "\n",
    "            Q_values[state][action] = expected_value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's iterate over our update rule to give our values a chance to converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1113,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(100):\n",
    "    update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And have a look at our new Q-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): {'up': -5.0, 'down': -1.42, 'left': -4.55, 'right': -4.55},\n",
       " (0, 1): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " (0, 2): {'up': -4.29, 'down': 1.3, 'left': -4.3, 'right': 1.36},\n",
       " (0, 3): {'up': -3.62, 'down': 2.3, 'left': 0.72, 'right': 1.78},\n",
       " (0, 4): {'up': -3.5, 'down': 2.83, 'left': 1.45, 'right': 2.12},\n",
       " (0, 5): {'up': -4.25, 'down': 3.23, 'left': 1.97, 'right': -4.07},\n",
       " (1, 0): {'up': -2.02, 'down': -0.58, 'left': -4.08, 'right': -4.08},\n",
       " (1, 1): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " (1, 2): {'up': 0.72, 'down': 1.05, 'left': -3.71, 'right': 2.21},\n",
       " (1, 3): {'up': 2.15, 'down': 1.64, 'left': 1.94, 'right': 2.68},\n",
       " (1, 4): {'up': 2.71, 'down': -3.32, 'left': 1.68, 'right': 3.23},\n",
       " (1, 5): {'up': 2.12, 'down': 4.83, 'left': 3.25, 'right': -3.08},\n",
       " (2, 0): {'up': -0.84, 'down': -0.38, 'left': -4.05, 'right': 0.58},\n",
       " (2, 1): {'up': -3.78, 'down': 0.73, 'left': -0.02, 'right': 0.87},\n",
       " (2, 2): {'up': 1.82, 'down': 0.85, 'left': 0.91, 'right': 1.42},\n",
       " (2, 3): {'up': 1.59, 'down': -4.34, 'left': 1.05, 'right': -4.26},\n",
       " (2, 4): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " (2, 5): {'up': 2.48, 'down': 7.0, 'left': -2.57, 'right': -2.57},\n",
       " (3, 0): {'up': -0.02, 'down': -4.44, 'left': -4.45, 'right': 0.06},\n",
       " (3, 1): {'up': 0.71, 'down': -3.92, 'left': -0.38, 'right': 0.21},\n",
       " (3, 2): {'up': 0.87, 'down': -4.44, 'left': 0.18, 'right': -4.34},\n",
       " (3, 3): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " (3, 4): {'up': -3.5, 'down': -3.5, 'left': -5.0, 'right': 7.0},\n",
       " (3, 5): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 0.0}}"
      ]
     },
     "execution_count": 1114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_values = {\n",
    "    state: {action: round(value, 2) for action, value in actions.items()}\n",
    "    for state, actions in Q_values.items()\n",
    "}\n",
    "Q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S # - - - -\n",
      "↓ # → → → ↓\n",
      "→ → ↑ ↑ # ↓\n",
      "- ↑ - # - F\n"
     ]
    }
   ],
   "source": [
    "visualise_path()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
