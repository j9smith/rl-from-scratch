{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Markov Decision Processes\n",
    "## 2.2 Introducing Randomness\n",
    "\n",
    "- Random transition probabilities $P(s'|s,a)$\n",
    "- Non-deterministic policies $\\pi(a|s)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deterministic Transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S # - - - -\n",
      "↓ # → → → ↓\n",
      "→ → ↑ - # ↓\n",
      "- - - # - F\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "gamma = 0.9\n",
    "\n",
    "grid = np.array([\n",
    "    ['S', '#', '-', '-', '-', '-'],\n",
    "    ['-', '#', '-', '-', '-', '-'],\n",
    "    ['-', '-', '-', '-', '#', '-'],\n",
    "    ['-', '-', '-', '#', '-', 'F']\n",
    "])\n",
    "\n",
    "rows, cols = grid.shape\n",
    "terminal_state = (3, 5)\n",
    "\n",
    "actions = ['up', 'down', 'left', 'right']\n",
    "\n",
    "rewards = { terminal_state: 10}\n",
    "\n",
    "Q_values = { (i, j): {a: 0.0 for a in actions} for i in range(rows) for j in range(cols) }\n",
    "\n",
    "def is_valid(state):\n",
    "    i, j = state\n",
    "    return 0 <= i < rows and 0 <= j < cols and grid[i, j] != '#'\n",
    "\n",
    "def get_next_state(state, action):\n",
    "    i, j = state\n",
    "    if action == 'up': next_state = (i-1, j)\n",
    "    elif action == 'down': next_state = (i+1, j)\n",
    "    elif action == 'left': next_state = (i, j-1)\n",
    "    elif action == 'right': next_state = (i, j+1)\n",
    "    \n",
    "    if is_valid(next_state): return True, next_state\n",
    "    else: return False, state\n",
    "\n",
    "def update():\n",
    "    for state in Q_values.keys():\n",
    "        if state == terminal_state or grid[state] == '#':\n",
    "            continue\n",
    "        for action in actions:\n",
    "            is_valid, next_state = get_next_state(state, action) \n",
    "            if is_valid:\n",
    "                reward = rewards.get(next_state, 0) \n",
    "                Q_values[state][action] = reward + gamma * max(Q_values[next_state].values())\n",
    "            else: \n",
    "                Q_values[state][action] = -5\n",
    "\n",
    "action_symbols = {\n",
    "    \"up\": \"↑\",\n",
    "    \"down\": \"↓\",\n",
    "    \"left\": \"←\",\n",
    "    \"right\": \"→\"\n",
    "}\n",
    "\n",
    "def get_best_action(state):\n",
    "    return max(Q_values[state], key=Q_values[state].get)\n",
    "\n",
    "def visualise_path():\n",
    "    grid_viz = grid.copy()\n",
    "\n",
    "    state = (0, 0)\n",
    "    path = []\n",
    "    max_steps = 1000\n",
    "\n",
    "    for _ in range(max_steps):\n",
    "        path.append(state)\n",
    "        if state == terminal_state:\n",
    "            break\n",
    "\n",
    "        best_action = get_best_action(state)\n",
    "        _, next_state = get_next_state(state, best_action)\n",
    "\n",
    "        if next_state == state:\n",
    "            pass\n",
    "\n",
    "        state = next_state\n",
    "    \n",
    "    for (i, j) in path:\n",
    "        if grid_viz[i, j] not in ['S', 'F']:\n",
    "            grid_viz[i, j] = action_symbols[get_best_action((i, j))]\n",
    "\n",
    "    for row in grid_viz:\n",
    "        print(\" \".join(row))\n",
    "        \n",
    "for i in range(50):\n",
    "    update()\n",
    "\n",
    "visualise_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): {'up': -5, 'down': 3.87, 'left': -5, 'right': -5},\n",
       " (0, 1): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " (0, 2): {'up': -5, 'down': 5.9, 'left': -5, 'right': 5.9},\n",
       " (0, 3): {'up': -5, 'down': 6.56, 'left': 5.31, 'right': 6.56},\n",
       " (0, 4): {'up': -5, 'down': 7.29, 'left': 5.9, 'right': 7.29},\n",
       " (0, 5): {'up': -5, 'down': 8.1, 'left': 6.56, 'right': -5},\n",
       " (1, 0): {'up': 3.49, 'down': 4.3, 'left': -5, 'right': -5},\n",
       " (1, 1): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " (1, 2): {'up': 5.31, 'down': 5.31, 'left': -5, 'right': 6.56},\n",
       " (1, 3): {'up': 5.9, 'down': 5.9, 'left': 5.9, 'right': 7.29},\n",
       " (1, 4): {'up': 6.56, 'down': -5, 'left': 6.56, 'right': 8.1},\n",
       " (1, 5): {'up': 7.29, 'down': 9.0, 'left': 7.29, 'right': -5},\n",
       " (2, 0): {'up': 3.87, 'down': 3.87, 'left': -5, 'right': 4.78},\n",
       " (2, 1): {'up': -5, 'down': 4.3, 'left': 4.3, 'right': 5.31},\n",
       " (2, 2): {'up': 5.9, 'down': 4.78, 'left': 4.78, 'right': 5.9},\n",
       " (2, 3): {'up': 6.56, 'down': -5, 'left': 5.31, 'right': -5},\n",
       " (2, 4): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " (2, 5): {'up': 8.1, 'down': 10.0, 'left': -5, 'right': -5},\n",
       " (3, 0): {'up': 4.3, 'down': -5, 'left': -5, 'right': 4.3},\n",
       " (3, 1): {'up': 4.78, 'down': -5, 'left': 3.87, 'right': 4.78},\n",
       " (3, 2): {'up': 5.31, 'down': -5, 'left': 4.3, 'right': -5},\n",
       " (3, 3): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " (3, 4): {'up': -5, 'down': -5, 'left': -5, 'right': 10.0},\n",
       " (3, 5): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 0.0}}"
      ]
     },
     "execution_count": 1152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_values = {\n",
    "    state: {action: round(value, 2) for action, value in actions.items()}\n",
    "    for state, actions in Q_values.items()\n",
    "}\n",
    "Q_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Deterministic Transitions\n",
    "Recall the formulation of a Markov decision process:\n",
    "$$(\\mathcal{S}, \\mathcal{A}, P, \\mathcal{R}, \\gamma)$$\n",
    "\n",
    "Where: \n",
    "- $\\mathcal{S}$ is the **state space**.\n",
    "- $\\mathcal{A}$ is the **action space**.\n",
    "- $P(s'|s,a)$ is the **transition function** - the probability of reaching state $s'$ by taking action $a$ in state $s$. \n",
    "- $R(s,a,s')$ is the reward received immediately after taking action $a$ in state $s$. \n",
    "- $\\gamma$ is a discount factor.\n",
    "\n",
    "Previously, in our gridworld, we worked by the assumption that state transitions were **completely deterministic**, i.e.:\n",
    "$$P(s'|s,a) = 1 \\quad \\forall s,a \\in \\mathcal{S}, \\mathcal{A}$$\n",
    "\n",
    "This means that every time we took a given action in a given state, we would move to the next state with 100% certainty every single time. For example, if we took action '$\\text{right}$' in state $(0,0)$, our agent will *always* move to state $(0,1)$. \n",
    "\n",
    "In the real world, state transitions are usually not determinstic, but instead are **probabilistic (stochastic)**. \n",
    "\n",
    "Let's go back to our gridworld. This time, the grid is slippery - instead of moving in our intended direction with 100% certainty, there is now a chance of slipping and moving in an unintended direction. \n",
    "\n",
    "Instead, when an action is chosen, we will: \n",
    "- Move in the intended direction with $p=0.8$.\n",
    "- Move left of the intended direction with $p=0.1$.\n",
    "- Move right of the intended direction with $p=0.1$.\n",
    "- Remain in the same state if the action results in an invalid move.\n",
    "\n",
    "Now our **state transition function $P(s'|s,a)$** becomes: \n",
    "$$P(s'|s,a)=\n",
    "\\begin{cases}\n",
    "0.8, & \\text{if } s' \\text{ is the intended direction from } s \\text{ given } a \\\\\n",
    "0.1, & \\text{if } s' \\text{ is to the left of the the intended direction from } s  \\\\\n",
    "0.1, & \\text{if } s' \\text{ is to the right of the intended direction from } s  \\\\\n",
    "0, & \\text{otherwise}\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid(state):\n",
    "    i, j = state\n",
    "    return 0 <= i < rows and 0 <= j < cols and grid[i, j] != '#'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update():\n",
    "    gamma = 0.9\n",
    "    for state in Q_values.keys():\n",
    "\n",
    "        if state == terminal_state or grid[state] == '#':\n",
    "            continue\n",
    "\n",
    "        for action in actions:\n",
    "            is_valid, next_state = get_next_state(state, action)\n",
    "            if is_valid:\n",
    "                reward = rewards.get(next_state, 0)\n",
    "                Q_values[state][action] = reward + gamma * max(Q_values[next_state].values())\n",
    "            else: \n",
    "                Q_values[state][action] = -5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To introduce this stochasticity into our gridworld, we introduce a new function: $\\text{get\\_possible\\_moves}$. This function will return a list of three tuples, with the $0$-th tuple of the list representing the intended action, and the other two tuples representing a perpendicular move to the intended direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_possible_moves(state, action):\n",
    "    i, j = state\n",
    "    if action == 'up':\n",
    "        return [(i-1, j), (i, j-1), (i, j+1)] # up, left, right\n",
    "    elif action == 'down':\n",
    "        return [(i+1, j), (i, j-1), (i, j+1)] # down, left, right\n",
    "    elif action == 'left':\n",
    "        return [(i, j-1), (i-1, j), (i+1, j)] # left, up, down\n",
    "    elif action == 'right':\n",
    "        return [(i, j+1), (i-1, j), (i+1, j)] # right, up, down "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then update our $\\text{get\\_next\\_state}$ function to first sample the actual move from our set of possible moves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1156,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "probabilities = [0.8, 0.1, 0.1] # p=0.8 of moving in the intended direction, (1-p)/2 for moving perpendicular\n",
    "\n",
    "def get_next_state(state, action):\n",
    "    possible_moves = get_possible_moves(state, action)\n",
    "\n",
    "    next_state = possible_moves[np.random.choice(3, p=probabilities)]\n",
    "\n",
    "    if is_valid(next_state):\n",
    "        return True, next_state\n",
    "    else: \n",
    "        return False, state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform our updates as before, using the equation: \n",
    "$$Q(s,a)=R + \\gamma \\cdot \\max_{a'}Q(s',a')$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, we just perform one update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1157,
   "metadata": {},
   "outputs": [],
   "source": [
    "update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can how visualise how an agent following the *optimal policy* $\\pi^*$ (that just means selecting the **best** action in each state every time) navigates the grid after just one update.\n",
    "\n",
    "Recall: \n",
    "$$\\pi^*= \\arg\\max_{a}Q(s,a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this equation to plot our agent's course across the grid. Look closely. We see that in state $(2,3)$ the agent has learned the optimal action '$\\text{left}$'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S # - - - -\n",
      "↓ # - → → ↓\n",
      "→ → → ← # ↓\n",
      "- - - # - F\n"
     ]
    }
   ],
   "source": [
    "visualise_path()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from the previous notebook that the update function iterates over all states and **retrieves the value of the next state corresponding to each action**. It will then use the value of this next state to update its belief about the value of its *own* state. \n",
    "\n",
    "Specifically, we look at this part of the $\\text{update}$ function: \n",
    "```python\n",
    "for action in actions:\n",
    "            is_valid, next_state = get_next_state(state, action)\n",
    "            reward = rewards.get(next_state, 0)\n",
    "                Q_values[state][action] = reward + gamma * max(Q_values[next_state].values())\n",
    "         \n",
    "```\n",
    "Recall how we laid out our state transition probabilities earlier: we said that with probability $0.8$, the action would transition state as intended, but with probability $0.2$, the action would transition state perpendicular to the intended. \n",
    "\n",
    "This means that during our update, the agent tested action '$\\text{left}$' in state $(2,3)$ and, with a probability of 10%, it moved upwards instead (i.e., ended up in state $(1,3)$). It then used the *value of this state* to update its belief about the value of $Q((2,3), \\text{left})$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'up': -5, 'down': -5, 'left': 5.3136, 'right': -5}"
      ]
     },
     "execution_count": 1159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_values[(2,3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do another iteration of our update rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1160,
   "metadata": {},
   "outputs": [],
   "source": [
    "update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And visualise the path that it has learned now. Already, it is failing to learn an optimal path to reach the terminal state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S # - ↓ ↓ ↓\n",
      "↓ # → → → ↑\n",
      "→ ↑ ↑ - # -\n",
      "- - - # - F\n"
     ]
    }
   ],
   "source": [
    "visualise_path()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the agent gets stuck in state $(1,5)$. Let's look at the Q-values for this state and see what the agent has learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'up': 7.29, 'down': 7.29, 'left': 7.29, 'right': -5}"
      ]
     },
     "execution_count": 1162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_values[(1,5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Q-values for actions '$\\text{up}$', '$\\text{down}$', and '$\\text{left}$' have been assigned equal value. With each value equal, the agent will simply pick the **first occurrence** of the maximum value, and so it selects action '$\\text{up}$'.\n",
    "\n",
    "We can also visualise what happens when we update our Q-values a few more times (remember, we want our Q-values to converge so we can't take the first update at face value):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S # - - - -\n",
      "↑ # - - - -\n",
      "- - - - # -\n",
      "- - - # - F\n"
     ]
    }
   ],
   "source": [
    "for _ in range(50):\n",
    "    update()\n",
    "visualise_path()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent gets stuck almost immediately as the Q-values converge to their 'true' values. This is the problem with taking a deterministic approach when dealing with probabilistic transitions: the Q-values fail to update in a manner that can be useful to us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): {'up': -5, 'down': 3.87, 'left': -5, 'right': -5},\n",
       " (0, 1): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " (0, 2): {'up': -5, 'down': 3.87, 'left': -5, 'right': -5},\n",
       " (0, 3): {'up': -5, 'down': 6.56, 'left': -5, 'right': 6.56},\n",
       " (0, 4): {'up': -5, 'down': 7.29, 'left': 7.29, 'right': 7.29},\n",
       " (0, 5): {'up': -5, 'down': 6.56, 'left': -5, 'right': -5},\n",
       " (1, 0): {'up': 3.49, 'down': 3.49, 'left': 3.49, 'right': -5},\n",
       " (1, 1): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " (1, 2): {'up': 3.49, 'down': 6.56, 'left': -5, 'right': 6.56},\n",
       " (1, 3): {'up': 5.9, 'down': 3.87, 'left': 5.9, 'right': 7.29},\n",
       " (1, 4): {'up': 6.56, 'down': -5, 'left': 6.56, 'right': 6.56},\n",
       " (1, 5): {'up': 5.9, 'down': 9.0, 'left': 5.9, 'right': 5.9},\n",
       " (2, 0): {'up': 3.14, 'down': 3.14, 'left': -5, 'right': 3.87},\n",
       " (2, 1): {'up': -5, 'down': 4.3, 'left': 3.49, 'right': 4.3},\n",
       " (2, 2): {'up': 5.9, 'down': 3.87, 'left': 3.87, 'right': 3.87},\n",
       " (2, 3): {'up': 6.56, 'down': -5, 'left': 5.31, 'right': -5},\n",
       " (2, 4): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " (2, 5): {'up': 8.1, 'down': 10.0, 'left': -5, 'right': -5},\n",
       " (3, 0): {'up': -5, 'down': -5, 'left': -5, 'right': 3.49},\n",
       " (3, 1): {'up': 3.87, 'down': -5, 'left': 3.14, 'right': 3.87},\n",
       " (3, 2): {'up': 5.31, 'down': -5, 'left': -5, 'right': -5},\n",
       " (3, 3): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " (3, 4): {'up': -5, 'down': 10.0, 'left': -5, 'right': 10.0},\n",
       " (3, 5): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 0.0}}"
      ]
     },
     "execution_count": 1164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_values = {\n",
    "    state: {action: round(value, 2) for action, value in actions.items()}\n",
    "    for state, actions in Q_values.items()\n",
    "}\n",
    "Q_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To address this, let's consider our update rule: \n",
    "$$Q(s,a)=R + \\gamma_ \\cdot \\max_{a'} Q(s',a')$$\n",
    "\n",
    "When we update our Q-value using this rule, we assume that the **the action always transitions to the best next state**. This means that the agent is not learning the **true expected value of the next state**. \n",
    "\n",
    "Consider an action which has a 90% chance of a reward of $10$, but a 10% chance of a penalty of $100$. Can we assign a value of $10$ to the action just because it is the most probable outcome? No - **we must weigh all outcomes by their probability** to get an accurate representation of the expected value: \n",
    "$$(0.9 * 10) + (0.1 * -100) = -1$$\n",
    "\n",
    "This is **exactly** what we need to consider when we're navigating our grid. Each action has a probability of transitioning to three separate states: \n",
    "- The one intended, with probability $0.8$.\n",
    "- To the left of the direction intended with probability $0.1$. \n",
    "- To the right of the direction intended with probability $0.1$. \n",
    "\n",
    "For each state that we could possibly land in, we need to consider both the **immediate reward** and the **value of that state**.\n",
    "\n",
    "Let's go through an example to see how that works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll work backwards from state $(3,5)$. We saw last time that the value of any action from the terminal state is $0$ because there are no future states and no more rewards to be gained., i.e.: \n",
    "$$Q((3,5),a) = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We step backwards to find $Q((2,5), \\text{down})$. We know that the immediate reward for taking this action is $10$, but we **must also consider** that when the agent takes this action, there is a 10% probability it will move **left**, into an obstacle, and a 10% probability that it will move **right**, out of bounds. Both of these transitions carry a **$-5$ penalty**, and so they must be factored into the value of this action. \n",
    "\n",
    "So we know: \n",
    "- There is a **$0.8$ probability of reward $10$** for moving down.\n",
    "- A **$0.1$ probability of reward $-5$** for moving left.\n",
    "- A **$0.1$ probability of reward $-5$** for moving right.\n",
    "\n",
    "We can sum these to find the immediate expected reward: \n",
    "$$\\mathbb{E}[R]=(0.8 \\times 10) + (0.1 \\times -5) + (0.1 \\times -5) = 7$$\n",
    "\n",
    "$$Q((2,5), \\text{down}) = 7 + 0.9 \\cdot 0 \\cdot 0 = 7$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of working with Q-values (action-values), we're going switch to working with **State Values $V(s)$** to keep things a little bit cleaner. \n",
    "\n",
    "Recall from the previous notebook that the **value of the state** is equal to the **value of the best action in that state**, i.e.: \n",
    "$$V(s) = \\max_a Q(s,a)$$\n",
    "\n",
    "Think of the value of a state as being the 'best opportunity' that state offers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now look at $V((1,5))$.\n",
    "$$V((1,5)) := Q((1,5), \\text{down})$$ \n",
    "\n",
    "We also want to consider the **value** of the next state $V(s')$ in our calculations, so we can propagate distant rewards back through our grid.\n",
    "\n",
    "We know: \n",
    "- There is a **$0.8$ probability of moving to a state with value $7$** but  **no immediate reward** by moving down.\n",
    "- A **$0.1$ probability of immediate reward $-5$** by moving left to a state with **no value**.\n",
    "- A **$0.1$ probability of immediate reward $-5$** by moving right to a state with **no value**.\n",
    "\n",
    "Remembering that we need to discount future values using discount factor $\\gamma=0.9$, our expected *value* is: \n",
    "$$(0.8 \\times [0 + (0.9 \\cdot 7)]) + (0.1 \\times (0+ -5)) + (0.1 \\times (0+-5)) = 4.04$$\n",
    "\n",
    "Each time, we're weighting both the **immediate reward and the discounted value of the next state** by the probability that taking a given action reaches that state, and then **summing across all possible states that may result from that action**.\n",
    "\n",
    "We represent this process mathematically as follows: \n",
    "$$V(s)=\\sum_{s'}P(s'|s,a)[R(s,a,s')+ \\gamma \\cdot V(s')]$$\n",
    "\n",
    "Equivalently: \n",
    "$$Q(s,a)=\\sum_{s'}P(s'|s,a)[R(s,a,s')+ \\gamma \\cdot \\max_{a'} Q(s',a')]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's change our $\\text{update}$ function to reflect this new update rule. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update():\n",
    "    gamma = 0.9\n",
    "    for state in Q_values.keys():\n",
    "\n",
    "        if state == terminal_state or grid[state] == '#':\n",
    "            continue\n",
    "\n",
    "        for action in actions:\n",
    "            possible_moves = get_possible_moves(state, action)\n",
    "\n",
    "            # We initialise the expected value at zero.\n",
    "            expected_value = 0\n",
    "\n",
    "            # We now sum over each possible move\n",
    "            for move, prob in zip(possible_moves, probabilities):\n",
    "                if is_valid(move):\n",
    "                    reward = rewards.get(move, 0)\n",
    "                    next_value = max(Q_values[move].values())\n",
    "\n",
    "                # A penalty of 5 and a zero-value for entering invalid states (obstacle/out of bounds)\n",
    "                else: reward = -5; next_value = 0\n",
    "\n",
    "                # We add P(s'|s, a) * [R + gamma * max Q(s', a')] to our expected value\n",
    "                expected_value += prob * (reward + gamma * next_value)\n",
    "\n",
    "            # Finally, we define our Q-value as the sum of the expected values of each possible move\n",
    "            Q_values[state][action] = expected_value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's iterate over our update rule to give our values a chance to converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "for _ in range(100):\n",
    "    update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And have a look at our new Q-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): {'up': -5.0, 'down': -1.42, 'left': -4.55, 'right': -4.55},\n",
       " (0, 1): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " (0, 2): {'up': -4.29, 'down': 1.3, 'left': -4.3, 'right': 1.36},\n",
       " (0, 3): {'up': -3.62, 'down': 2.3, 'left': 0.72, 'right': 1.78},\n",
       " (0, 4): {'up': -3.5, 'down': 2.83, 'left': 1.45, 'right': 2.12},\n",
       " (0, 5): {'up': -4.25, 'down': 3.23, 'left': 1.97, 'right': -4.07},\n",
       " (1, 0): {'up': -2.02, 'down': -0.58, 'left': -4.08, 'right': -4.08},\n",
       " (1, 1): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " (1, 2): {'up': 0.72, 'down': 1.05, 'left': -3.71, 'right': 2.21},\n",
       " (1, 3): {'up': 2.15, 'down': 1.64, 'left': 1.94, 'right': 2.68},\n",
       " (1, 4): {'up': 2.71, 'down': -3.32, 'left': 1.68, 'right': 3.23},\n",
       " (1, 5): {'up': 2.12, 'down': 4.83, 'left': 3.25, 'right': -3.08},\n",
       " (2, 0): {'up': -0.84, 'down': -0.38, 'left': -4.05, 'right': 0.58},\n",
       " (2, 1): {'up': -3.78, 'down': 0.73, 'left': -0.02, 'right': 0.87},\n",
       " (2, 2): {'up': 1.82, 'down': 0.85, 'left': 0.91, 'right': 1.42},\n",
       " (2, 3): {'up': 1.59, 'down': -4.34, 'left': 1.05, 'right': -4.26},\n",
       " (2, 4): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " (2, 5): {'up': 2.48, 'down': 7.0, 'left': -2.57, 'right': -2.57},\n",
       " (3, 0): {'up': -0.02, 'down': -4.44, 'left': -4.45, 'right': 0.06},\n",
       " (3, 1): {'up': 0.71, 'down': -3.92, 'left': -0.38, 'right': 0.21},\n",
       " (3, 2): {'up': 0.87, 'down': -4.44, 'left': 0.18, 'right': -4.34},\n",
       " (3, 3): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " (3, 4): {'up': -3.5, 'down': -3.5, 'left': -5.0, 'right': 7.0},\n",
       " (3, 5): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 0.0}}"
      ]
     },
     "execution_count": 1167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_values = {\n",
    "    state: {action: round(value, 2) for action, value in actions.items()}\n",
    "    for state, actions in Q_values.items()\n",
    "}\n",
    "Q_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now see how our agent navigates our grid. We can see some slipping, but the agent manages to get itself back on the right track."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S # - - - -\n",
      "↓ # → → → ↓\n",
      "→ → ↑ - # ↓\n",
      "- - - # - F\n"
     ]
    }
   ],
   "source": [
    "visualise_path()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Policies\n",
    "\n",
    "Previously, we have seen how the agent navigates the grid using a **deterministic policy** (specifically the optimal policy), where it simply picks the action with the highest Q-value in each state: \n",
    "$$\\pi^*(a|s)=\\arg \\max_{a}Q(s,a)$$\n",
    "\n",
    "We will now consider situations in which the agent will navigate the grid using a **Stochastic Policy**, where the **next action is sampled from a probability distribution across the action space**: \n",
    "$$\\pi(a|s)=P(a|s)$$\n",
    "where the policy for taking action $a$ in state $s$ is given by the probability for taking action $a$ in state $s$.\n",
    "\n",
    "We briefly saw in the previous notebook how we can convert Q-values into probabilities by using a softmax distribution: \n",
    "$$P(a)=\\frac{e^{Q(a)}}{\\sum_{a}e^{Q(a)}}$$\n",
    "\n",
    "We can also introduce a new temperature parameter, $\\tau$, to this function to control the shape of the distribution:\n",
    "$$P(a)=\\frac{e^{\\frac{Q(a)}{\\tau}}}{\\sum_{a}e^{\\frac{Q(a)}{\\tau}}}$$\n",
    "- A high $\\tau$ results in a more uniform probability. \n",
    "- A low $\\tau$ results in a more determinstic policy, with a preference towards optimal values. \n",
    "This is useful for controlling **exploration vs. exploitation**. \n",
    "\n",
    "Let's see how that works by looking at the Q-values for state $(2,0)$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'up': -0.84, 'down': -0.38, 'left': -4.05, 'right': 0.58}"
      ]
     },
     "execution_count": 1191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_values[(2,0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's map those to probabilities in the range $[0,1]$ using our softmax function, and experiment with different values of our temperature parameter tau, $\\tau$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tau = 1: {'up': 0.148, 'down': 0.234, 'left': 0.006, 'right': 0.612}\n",
      "Tau = 10: {'up': 0.255, 'down': 0.267, 'left': 0.185, 'right': 0.294}\n",
      "Tau = 0.1: {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 1.0}\n"
     ]
    }
   ],
   "source": [
    "def softmax(q_values, tau=1.0):\n",
    "    values = np.array(list(q_values.values()))\n",
    "    exp = np.exp(values/tau)\n",
    "    probabilities = (exp / np.sum(exp)).tolist()\n",
    "\n",
    "    return ({action: round(prob,3) for action, prob in zip(q_values.keys(), probabilities)})\n",
    "\n",
    "print(f\"Tau = 1: {softmax(Q_values[(2,0)], 1)}\")\n",
    "print(f\"Tau = 10: {softmax(Q_values[(2,0)], 10)}\")\n",
    "print(f\"Tau = 0.1: {softmax(Q_values[(2,0)], 0.1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stochastic Policies and Bellman Equations\n",
    "We have previously considered only **deterministically selecting the best action** when calculating our state values and action values: \n",
    "$$V^{\\pi^*}(s)=\\sum_{s'}P(s'|s,a)[R(s,a,s')+ \\gamma \\cdot V^{\\pi^*}(s')]$$\n",
    "\n",
    "Equivalently: \n",
    "$$Q^{\\pi^*}(s,a)=\\sum_{s'}P(s'|s,a)[R(s,a,s')+ \\gamma \\cdot \\max_{a'} Q(s',a')]$$\n",
    "where we introduce new notation $V^{\\pi^*}(s)$ and $Q^{\\pi^*}(s,a)$ to denote the state values and action values **calculated under the assumption of following the optimal policy $\\pi^*$**. \n",
    "\n",
    "In similar fashion to accounting for our stochastic state transitions, we also now **need to weigh the probability of each action in each state**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of following the **optimal policy update rule**: \n",
    "$$Q^{\\pi^*}(s,a)=\\sum_{s'}P(s'|s,a)[R(s,a,s')+ \\gamma \\cdot \\max_{a'} Q(s',a')]$$\n",
    "\n",
    "Under stochastic policy $\\pi$, we'd want to set the value of the action $Q(s,a)$ equal to:\n",
    "$$Q^\\pi(s,a)=\\sum_{s'}P(s'|s,a)[R(s,a,s')+ \\gamma \\cdot \\text{???}]$$\n",
    "\n",
    "Instead of considering only $\\max_{a'}Q(s',a')$ (as we did when following the *optimal* policy), we must now factor in **the probability of each action being selected**, i.e., we must consider $\\pi(a|s)$, and weigh the value of $Q^\\pi(s',a')$ accordingly. \n",
    "\n",
    "To do this, we simply **multiply the action value of each action by the probability of it happening and sum them together**:\n",
    "$$Q^\\pi(s,a)=\\sum_{s'} P(s'|s,a)[R(s,a,s') + \\gamma \\cdot \\sum_{a'} \\pi(a'|s') \\cdot Q^\\pi(s',a')]$$ \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
