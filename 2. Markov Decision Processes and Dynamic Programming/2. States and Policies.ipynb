{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction to RL\n",
    "### 1.2 States, MDPs, and Policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State\n",
    "We define our state space, $\\mathcal{S}$, as each point in the grid across which our agent will traverse. $\\text{S}$ represents the starting state, $\\text{F}$ represents the terminal state which will dispense a reward, and $\\text{\\#}$ show obstacles. \n",
    "\n",
    "The **current state**, $s$ is our agent's location in the **state space**, $s \\in \\mathcal{S}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = np.array([\n",
    "    ['S', '-', '-', '#'],\n",
    "    ['-', '#', '-', '-'],\n",
    "    ['-', '-', '-', '#'],\n",
    "    ['#', '-', '-', 'F']\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define our action space $\\mathcal{A}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = ['up', 'down', 'left', 'right']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define our reward as $10$ for finishing in state $\\text{F}$ which is located at index $(3,3)$ of our array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = { (3, 3): 10}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the bandit problem, we initialise a dictionary of zeroed action values, but now introduce the dimension of **state**. \n",
    "\n",
    "Recall that in the bandit problem, action values are given by:\n",
    "$$q(a)=\\mathbb{E}[R|A=a]$$\n",
    "\n",
    "This means that the expected reward depends *only on the action taken*. \n",
    "\n",
    "Now that we have introduced **state**, rewards are determined not by the action taken, but also by the *state in which the action was taken*: \n",
    "$$q(s,a) = \\mathbb{E}[R| S=s, A=a]$$\n",
    "\n",
    "If you're struggling to make sense of that, take a look at the dictionary print-out below - we've simply added another dimension. Now, action values are stored for each **state-action pair**, instead of for just actions alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " (0, 1): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " (0, 2): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " (0, 3): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " (1, 0): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " (1, 1): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " (1, 2): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " (1, 3): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " (2, 0): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " (2, 1): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " (2, 2): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " (2, 3): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " (3, 0): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " (3, 1): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " (3, 2): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " (3, 3): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 0.0}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_values = { (i, j): {a: 0.0 for a in actions} for i in range(4) for j in range(4) }\n",
    "Q_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function for checking it the state the agent is trying to move to is valid - we don't want it going out of bounds or into an obstacle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid(state):\n",
    "    i, j = state\n",
    "    return 0 <= i < 4 and 0 <= j < 4 and grid[i, j] != '#'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define a function for retrieving the next state, where we adjust the indices of our array, check their validity, and then return the new state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_state(state, action):\n",
    "    i, j = state\n",
    "    if action == 'up': next_state = (i-1, j)\n",
    "    elif action == 'down': next_state = (i+1, j)\n",
    "    elif action == 'left': next_state = (i, j-1)\n",
    "    elif action == 'right': next_state = (i, j+1)\n",
    "    \n",
    "    if is_valid(next_state): return next_state\n",
    "    else: return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we define a $\\text{play}$ function. This initialises our agent at the starting position $\\text{S}$, i.e., $(0,0)$. Our agent will then randomly select actions and update their action-values (Q-values) as it progresses through the grid, and terminate when it reaches position $\\text{F}$, i.e., $(3,3)$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(): \n",
    "    state = (0, 0)\n",
    "    while state != (3, 3): \n",
    "        action = random.choice(actions)\n",
    "        next_state = get_next_state(state, action)\n",
    "        reward = rewards.get(next_state, 0)\n",
    "        \n",
    "        Q_values[state][action] += reward\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you think our action value dictionary is going to look like after we have finished?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "play()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look what our agent has learned by checking the table of Q-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " (0, 1): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " (0, 2): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " (0, 3): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " (1, 0): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " (1, 1): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " (1, 2): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " (1, 3): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " (2, 0): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " (2, 1): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " (2, 2): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " (2, 3): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " (3, 0): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " (3, 1): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " (3, 2): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 10.0},\n",
       " (3, 3): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 0.0}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent *hasn't learned anything* about how to navigate the grid. It has randomly selected actions in each state until it happened upon the terminal state. Instead of improving its decision-making, it has brute-forced its way through, blindly repeating the process every time it plays.\n",
    "\n",
    "What we *want to do* is to propagate the reward backward through the grid, ensuring that each state-action pair receives a **small update** according to how much it contributed to the final reward. \n",
    "\n",
    "This is where **Markov Decision Processes (MDPs)** and **Dynamic Programming** come in. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Decision Processes (MDPs)\n",
    "A **Markov Decision Process (MDP)** is a **model for sequential decision making when outcomes are uncertain**. \n",
    "\n",
    "We can consider our grid as a sequence of decisions - with every state we are presented, we have four possible actions to take (up, down, left, right), and we must try to take the most optimal action.\n",
    "\n",
    "An MDP is defined as a tuple: \n",
    "$$(\\mathcal{S}, \\mathcal{A}, P, \\mathcal{R}, \\gamma)$$\n",
    "\n",
    "Where: \n",
    "- $\\mathcal{S}$ is the **state space**.\n",
    "- $\\mathcal{A}$ is the **action space**.\n",
    "- $P(s'|s,a)$ is the **transition function** - the probability of reaching state $s'$ by taking action $a$ in state $s$. \n",
    "- $R(s,a,s')$ is the reward received immediately after taking action $a$ in state $s$. \n",
    "- $\\gamma$ is a discount factor, which allows us to control how much future rewards matter for the current decision. \n",
    "\n",
    "Unlike the bandit approach we explored earlier, a Markov decision process doesn't just consider immediate rewards - it also considers future rewards and how different actions lead to different states. \n",
    "\n",
    "To be able to model our problem as a Markov decision process, it must fulfill two criteria: \n",
    "- It must *not* violate the Markov property.\n",
    "- We need *complete* knowledge of the MDP components $(\\mathcal{S}, \\mathcal{A}, P, \\mathcal{R}, \\gamma)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Markov Property\n",
    "We can **only model a problem as an MDP if it satisfies the Markov Property**. \n",
    "\n",
    "The Markov property states that **the future state depends only on the current state and action, and not on the full history of past states**. \n",
    "\n",
    "Our gridworld fulfills this property because: \n",
    "- If our agent is in state $(1,2)$, the only *relevant information* for deciding what to do next is what actions are available from $(1,2)$. \n",
    "- It *does not matter* how the agent got to state $(1,2)$, and so past moves have no bearing on future decisions. \n",
    "\n",
    "A game like blackjack would **violate** the Markov property because: \n",
    "- Future states depend on past decisions. \n",
    "- The best decision (e.g., hit or stick) depends not just on the current hand, but also **what cards have already been played** - thus, past decisions affect future states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to check if we have complete knowledge of the components $(\\mathcal{S}, \\mathcal{A}, P, \\mathcal{R}, \\gamma)$.\n",
    "\n",
    "We know $\\mathcal{S}$, the state space, because we defined it earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = np.array([\n",
    "    ['S', '-', '-', '#'],\n",
    "    ['-', '#', '-', '-'],\n",
    "    ['-', '-', '-', '#'],\n",
    "    ['#', '-', '-', 'F']\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know $\\mathcal{A}$, the action space: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = ['up', 'down', 'left', 'right']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know $P(s'|s,a)$, the transition function. Moving around our grid is **deterministic** - if we take action '$\\text{right}$' in state $(0,0)$, we're going to move to state $(0,1)$ with 100% certainty, i.e.:\n",
    "$$P(s'|s,a) = 1 \\quad \\forall s,a \\in \\mathcal{S}, \\mathcal{A}$$\n",
    "\n",
    "This **does not always hold**. In many environments, transitions are stochastic, meaning that the same action can lead to different outcomes with some probability. We'll explore these later. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know $R(s,a,s')$, the reward for taking action $a$ in state $s$ leading to the new state $s'$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = { (3, 3): 10}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\gamma$ (gamma), the discount factor, is defined by us, so we know that too.\n",
    "\n",
    "Since our gridworld satisfies the Markov property, and we have all five components of the MDP (states, actions, transition probabilities, rewards, and $\\gamma$) we can formally model our problem as a Markov decision process.\n",
    "\n",
    "Our goal is now to solve this MDP, meaning we must find the best action to take in each state to maximise our reward.\n",
    "\n",
    "To solve our MDP, we turn to **Dynamic Programming**, a method that allows us to systematically compute optimal values and decisions using the **Bellman Equation**.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Programming\n",
    "Dynamic programming is a technique used in computer science (among other fields) to break problems down into small overlapping subproblems and solve them recursively. \n",
    "\n",
    "In our case, this means breaking down decision-making in our gridworld into localised state-action updates, where each Q-value depends on the expected rewards and future values of neighbouring states. \n",
    "\n",
    "If this doesn't quite make sense yet, consider how we previously tried to optimise our gridworld by **looking at the whole grid at once**, and using information gained at the very end to update our Q-values, resulting only in states directly adjacent to the terminal state receiving any sort of update. \n",
    "\n",
    "A dynamic programming approach means that we will look at **each cell in turn**, propagating backwards through our grid and updating each cell in turn using information from the previously visited cells, e.g.: \n",
    "- We receive a reward from cell/state $(3,3)$. \n",
    "- We use that value to update cell/state $(3,2)$.\n",
    "- We use that value to update cell/state $(2,2)$ and $(3,1)$.\n",
    "- ... \n",
    "- We update our starting cell $\\text{S}$, $(0,0)$.\n",
    "\n",
    "So how can we do this in practice? How do we use information from our terminal state to update our beliefs about earlier decisions? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Bellman Equation \n",
    "As described earlier, we want to *propagate the reward backwards* through the grid. The process for doing this is described by the **Bellman Equation**. To understand it, we'll first run through it by hand. \n",
    "\n",
    "Let's revisit our state-action value function: \n",
    "$$Q(s,a)=\\mathbb{E}(R|S=s, A=a)$$\n",
    "\n",
    "And our grid: \n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "\\text{S} & - & - & \\# \\\\\n",
    "- & \\# & - & - \\\\\n",
    "- & - & - & \\# \\\\\n",
    "\\# & - & - & \\text{F}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "And our Q-value table: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " (0, 1): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " (0, 2): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " (0, 3): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " (1, 0): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " (1, 1): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " (1, 2): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " (1, 3): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " (2, 0): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " (2, 1): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " (2, 2): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " (2, 3): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " (3, 0): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " (3, 1): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " (3, 2): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " (3, 3): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 0.0}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_values = { (i, j): {a: 0.0 for a in actions} for i in range(4) for j in range(4) }\n",
    "Q_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start from our terminal state, $(3,3)$. Because $(3,3)$ is our terminal state, we know that taking **any action** in this state **offers no reward**: \n",
    "$$Q((3,3), a) = 0$$\n",
    "\n",
    "Now let's consider state $(3,2)$. Because $(3,3)$ is the goal, the reward for moving $\\text{right}$ in state $(3,2)$ is $10$. This means we can update $Q((3,2), \\text{right})$ as: \n",
    "$$Q((3,2), \\text{right})= 10$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 10}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_values[(3,2)]['right'] = 10\n",
    "Q_values[(3,2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about the states adjacent to $(3,2)$? \n",
    "\n",
    "Let's look at $Q((3,1), a)$ first. We know that we can reach state $(3,2)$ from state $(3,1)$ by taking action $\\text{right}$. We want to propagate the reward received by taking $a=\\text{right}$ in state $(3,2)$ backwards. It wouldn't make sense to pass this reward backwards directly, so we instead apply our **discount factor, $\\gamma$**:\n",
    "$$Q((3,1), \\text{right}) = \\gamma \\cdot Q((3,2), \\text{right})$$\n",
    "\n",
    "We'll set our discount factor $\\gamma=0.9$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now passed *some of the value of the terminal reward* one step backwards through our trajectory. We now have a non-zero Q-value for an action in state $(3,1)$ that can inform our agent's decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 9.0}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_values[(3,1)]['right'] = gamma * Q_values[(3,2)]['right']\n",
    "Q_values[(3,1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's step backwards again to $Q((2,1), \\text{down})$: \n",
    "$$Q((2,1), \\text{down}) = \\gamma \\cdot Q((3,1), \\text{right})$$\n",
    "\n",
    "Again, we have propagate *some of the value* of the terminal reward backwards through our grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'up': 0.0, 'down': 8.1, 'left': 0.0, 'right': 0.0}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_values[(2,1)]['down'] = gamma * Q_values[(3,1)]['right']\n",
    "Q_values[(2,1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the same again for each step back through $(2,0)$, $(1,0)$, and $(0,0)$: \n",
    "$$Q((2,0), \\text{right}) = \\gamma \\cdot Q((2,1), \\text{down})$$\n",
    "$$Q((1,0), \\text{down}) = \\gamma \\cdot Q((2,0), \\text{right})$$\n",
    "$$Q((0,0), \\text{down}) = \\gamma \\cdot Q((1,0), \\text{down})$$\n",
    "\n",
    "Remember, we're just walking backwards through our grid and copying over a fraction of the reward at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_values[(2,0)]['right'] = gamma * Q_values[(2,1)]['down']\n",
    "Q_values[(1,0)]['down'] = gamma * Q_values[(2,0)]['right']\n",
    "Q_values[(0,0)]['down'] = gamma * Q_values[(1,0)]['down']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now see how the Q-values have propagated back through our grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'up': 0.0, 'down': 5.9049000000000005, 'left': 0.0, 'right': 0.0}\n",
      "{'up': 0.0, 'down': 6.561, 'left': 0.0, 'right': 0.0}\n",
      "{'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 7.29}\n",
      "{'up': 0.0, 'down': 8.1, 'left': 0.0, 'right': 0.0}\n",
      "{'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 9.0}\n",
      "{'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 10}\n",
      "{'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 0.0}\n"
     ]
    }
   ],
   "source": [
    "visited_states =[(0,0), (1,0), (2,0), (2,1), (3,1), (3,2), (3,3)]\n",
    "\n",
    "for state in visited_states:\n",
    "    print(Q_values[state])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use these learned Q-values to enforce a path for our agent to take through the grid, simply by choosing the highest action-value for each state.\n",
    "\n",
    "Ideally, we don't want to solve this by hand for every problem. We need a more generalisable update rule - one where we don't need to pre-define an action to take in each stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at our Q-values for state $(3,2)$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 10}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_values[(3,2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider our update for the adjacent state:\n",
    "$$Q((3,1), \\text{right}) = \\gamma \\cdot Q((3,2), \\text{right})$$\n",
    "\n",
    "We chose the **action corresponding to the highest Q-value in the next state**, i.e., we chose:\n",
    "$$\\max_{a'} Q(s', a')$$\n",
    "\n",
    "We did this for every single state we propagated backwards through. \n",
    "\n",
    "This allows us to derive a general update rule: \n",
    "$$Q(s,a) = \\gamma \\cdot \\max_{a'} Q(s', a')$$\n",
    "\n",
    "We can also consider the case where we receive **intermediate rewards**. While most states will not have any reward, i.e., $R=0$, what if, for example, state $(1,1)$ had a power-up which gave us a small reward of $R=1$? We'd want to account for this in our Q-values, and so we would simply add the value of the reward on to any state-action pairs that lead to state $(1,1)$.  \n",
    "\n",
    "We can finalise our update rule as: \n",
    "$$Q(s,a) = R + \\gamma \\cdot \\max_{a'} Q(s', a')$$\n",
    "\n",
    "This is known as the **Bellman equation** and is example of **Dynamic Programming** because it involves overlapping subproblems - $Q(s,a)$ becomes $Q(s', a')$ for the next state-action value we're trying to calculate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Return \n",
    "Because we're talking about distant rewards, it's no longer appropriate to describe our Q-values as the 'expected reward' for taking an action $a$ in state $s$. \n",
    "\n",
    "Let's look at our update rule: \n",
    "$$Q(s,a) = R + \\gamma \\cdot \\max_{a'} Q(s', a')$$\n",
    "\n",
    "We're taking the **immediate reward**, $R$ (if it exists) of the next state plus a portion of the **reward propagated backwards from future states**. \n",
    "\n",
    "Let's look at state $(3,2)$ again: \n",
    "$$\\begin{align}Q((3,2), \\text{right}) &= 10 + \\gamma \\cdot Q((3,3), a)\\\\&=10 + 0.9\\cdot 0\n",
    "\\end{align}$$\n",
    "\n",
    "The Q-value of action '$\\text{right}$' in state $(3,2)$ is equal to the immediate reward $R$ plus the a portion of the reward from taking the best action in the next state, i.e., one step ahead - $R_{t+1}$: \n",
    "\n",
    "$$Q((3,2), \\text{right})= R + \\gamma \\cdot R_{t+1}$$\n",
    "\n",
    "Similarly for state $(3,1)$: \n",
    "$$Q((3,1), \\text{right}) = R + \\gamma \\cdot (R_{t+1} + \\gamma\\cdot R_{t+2})$$\n",
    "\n",
    "State $(2,1)$:\n",
    "$$Q((2,1), \\text{down}) = R + \\gamma (R_{t+1} + \\gamma \\cdot (R_{t+2} + \\gamma (R_{t+3})))$$\n",
    "\n",
    "Do you see a pattern here? \n",
    "\n",
    "This is where we introduce the concept of **Return**, denoted by $G_t$. Instead of $Q(s,a)$ describing an *immediate reward*, it is instead describing **the total accumulated reward from a given state onwards**: \n",
    "$$G_t=R_t + \\gamma R_{t+1} + \\gamma^2 R_{t+2} + \\gamma^3 R_{t+3} + ... + \\gamma^k R_{t+k}$$\n",
    "\n",
    "Or: \n",
    "$$G_t = \\sum_{k=0}^\\infty \\gamma^k R_{t+k}$$\n",
    "\n",
    "Previously, we defined our Q-values (action-values) as the **expected reward for taking action $a$ in state $s$**, i.e.: \n",
    "$$Q(s,a)=\\mathbb{E}[R|S=s,A=a]$$\n",
    "\n",
    "Now that we are factoring in **expected future rewards**, we redefine our Q-values as: \n",
    "$$Q(s,a)=\\mathbb{E}[G_t|S=s, A=a]$$\n",
    "Equivalently:\n",
    "$$Q(s,a)=\\mathbb{E}[\\sum_{k=0}^\\infty \\gamma^k R_{t+k} | S=s, A=a]$$\n",
    "\n",
    "Now we described our action-value function $Q(s,a)$ as the **expected return when starting in state $s$ and taking action $a$, and selecting every optimal action thereafter**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State-Value Function\n",
    "Similarly to the **action-value function**, Q(s,a), which describes **the value of taking a given action in a given state**, we can also define a new concept: the **State-Value Function**, expressed as **$V(s)$**, which describes the **value of being in a given state**.\n",
    "\n",
    "The state-value function is defined by the **maximum expected return achievable** by selecting the best possible action in that state: \n",
    "\n",
    "$$V(s)=\\max_{a}Q(s,a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back to our grid\n",
    "Let's use the Bellman equation to build a solution to tackle our gridworld. We set our problem up as earlier: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = np.array([\n",
    "    ['S', '-', '-', '#'],\n",
    "    ['-', '#', '-', '-'],\n",
    "    ['-', '-', '-', '#'],\n",
    "    ['#', '-', '-', 'F']\n",
    "])\n",
    "\n",
    "actions = ['up', 'down', 'left', 'right']\n",
    "\n",
    "rewards = { (3, 3): 10}\n",
    "\n",
    "Q_values = { (i, j): {a: 0.0 for a in actions} for i in range(4) for j in range(4) }\n",
    "\n",
    "def is_valid(state):\n",
    "    i, j = state\n",
    "    return 0 <= i < 4 and 0 <= j < 4 and grid[i, j] != '#'\n",
    "\n",
    "def get_next_state(state, action):\n",
    "    i, j = state\n",
    "    if action == 'up': next_state = (i-1, j)\n",
    "    elif action == 'down': next_state = (i+1, j)\n",
    "    elif action == 'left': next_state = (i, j-1)\n",
    "    elif action == 'right': next_state = (i, j+1)\n",
    "    \n",
    "    if is_valid(next_state): return True, next_state\n",
    "    else: return False, state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But now replace the $\\text{play}$ function with an $\\text{update}$ function to propagate our reward from the terminal state back through the grid using the Bellman equation we just derived."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update():\n",
    "    # We first iterate over every state\n",
    "    for state in Q_values.keys():\n",
    "\n",
    "        # We can skip the terminal state and any obstacle\n",
    "        if state == (3, 3) or grid[state] == '#':\n",
    "            continue\n",
    "\n",
    "        # We iterate over every action in each state\n",
    "        for action in actions:\n",
    "            is_valid, next_state = get_next_state(state, action) # For each state, we find every next state reachable\n",
    "            if is_valid:\n",
    "                reward = rewards.get(next_state, 0) # And check if they offer a reward\n",
    "                \n",
    "                # We update the value of the current state-action according to the \n",
    "                # maximum state-action value in the next state\n",
    "                # Bellman equation: Q(s,a) = R + gamma * max Q(s', a')\n",
    "                Q_values[state][action] = reward + gamma * max(Q_values[next_state].values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We iterate over our update function $10$ times to allow our values to propagate backwards through our grid and converge. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(10):\n",
    "    update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now check to see what our agent has learned by examining our Q-value dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): {'up': 0.0, 'down': 5.9, 'left': 0.0, 'right': 5.9},\n",
       " (0, 1): {'up': 0.0, 'down': 0.0, 'left': 5.31, 'right': 6.56},\n",
       " (0, 2): {'up': 0.0, 'down': 7.29, 'left': 5.9, 'right': 0.0},\n",
       " (0, 3): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " (1, 0): {'up': 5.31, 'down': 6.56, 'left': 0.0, 'right': 0.0},\n",
       " (1, 1): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " (1, 2): {'up': 6.56, 'down': 8.1, 'left': 0.0, 'right': 6.56},\n",
       " (1, 3): {'up': 0.0, 'down': 0.0, 'left': 7.29, 'right': 0.0},\n",
       " (2, 0): {'up': 5.9, 'down': 0.0, 'left': 0.0, 'right': 7.29},\n",
       " (2, 1): {'up': 0.0, 'down': 8.1, 'left': 6.56, 'right': 8.1},\n",
       " (2, 2): {'up': 7.29, 'down': 9.0, 'left': 7.29, 'right': 0.0},\n",
       " (2, 3): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " (3, 0): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 0.0},\n",
       " (3, 1): {'up': 7.29, 'down': 0.0, 'left': 0.0, 'right': 9.0},\n",
       " (3, 2): {'up': 8.1, 'down': 0.0, 'left': 8.1, 'right': 10.0},\n",
       " (3, 3): {'up': 0.0, 'down': 0.0, 'left': 0.0, 'right': 0.0}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_values = {\n",
    "    state: {action: round(value, 2) for action, value in actions.items()}\n",
    "    for state, actions in Q_values.items()\n",
    "}\n",
    "Q_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there are some values that don't seem to make a lot of sense. Why, in state $(3,2)$, would we assign a Q-value of $8.1$ to '$\\text{left}$'? That's moving us in the opposite direction to our reward!\n",
    "\n",
    "Consider how our values are updated: we take the **Q-value of the most rewarding action in the adjacent state and use that to assign a Q-value to the action we took to get there**. This means that if we have already updated our grid once and assigned $Q((3,1), \\text{right})=9$, the next time we run our update rule it will be: \n",
    "$$\\begin{align}Q((3,2), \\text{left})&=R+\\gamma \\cdot Q((3,1), \\text{right})\\\\\n",
    "&= 0 + 0.9 \\cdot 9 \\\\\n",
    "&= 8.1\n",
    "\\end{align}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our Q-values, how can our agent use them to navigate the grid? \n",
    "\n",
    "This is where the concept of **policy** comes in. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy\n",
    "A policy, denoted by $\\pi$, defines the strategy that an agent follows to decide which action to take in a given state. \n",
    "\n",
    "There are two types of policy: \n",
    "- **Deterministic** policies\n",
    "- **Stochastic** policies\n",
    "\n",
    "#### Deterministic Policies\n",
    "A deterministic policy will select the same action every time in a given state:\n",
    "$$\\pi(s)=a$$\n",
    "\n",
    "Consider our gridworld and the accompanying Q-values we learned using the Bellman equation. \n",
    "\n",
    "If **every time** our agent navigated the grid, it selected each **action with the highest Q-value**, we would say that it is following a deterministic policy.\n",
    "\n",
    "Selecting the action with the highest Q-value every time is known as the **optimal policy**, which is denoted by $\\pi^*$: \n",
    "$$\\pi^*= \\arg\\max_{a}Q(s,a)$$\n",
    "\n",
    "When following the optimum policy, we can retrieve the value of any state-action pair (Q-value) as:\n",
    "$$Q(s,a)\\mathbb{E}[\\sum_{t=0}^\\infty \\gamma^t R_t |S = s, A= a]$$\n",
    "where $t$ is each step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualise the route our agent takes across our gridworld when following the optimal policy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S - - #\n",
      "↓ # - -\n",
      "→ ↓ - #\n",
      "# → → F\n"
     ]
    }
   ],
   "source": [
    "action_symbols = {\n",
    "    \"up\": \"↑\",\n",
    "    \"down\": \"↓\",\n",
    "    \"left\": \"←\",\n",
    "    \"right\": \"→\"\n",
    "}\n",
    "\n",
    "def get_best_action(state):\n",
    "    return max(Q_values[state], key=Q_values[state].get)\n",
    "\n",
    "def visualise_path():\n",
    "    grid_viz = grid.copy()\n",
    "\n",
    "    state = (0, 0)\n",
    "    path = []\n",
    "    max_steps = 20\n",
    "\n",
    "    for _ in range(max_steps):\n",
    "        path.append(state)\n",
    "        if state == (3,3):\n",
    "            break\n",
    "\n",
    "        best_action = get_best_action(state)\n",
    "        _, next_state = get_next_state(state, best_action)\n",
    "\n",
    "        if next_state == state:\n",
    "            break\n",
    "\n",
    "        state = next_state\n",
    "    \n",
    "    for (i, j) in path:\n",
    "        if grid_viz[i, j] not in ['S', 'F']:\n",
    "            grid_viz[i, j] = action_symbols[get_best_action((i, j))]\n",
    "\n",
    "    for row in grid_viz:\n",
    "        print(\" \".join(row))\n",
    "\n",
    "visualise_path()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stochastic Policies\n",
    "A stochastic policy assigns a **probability distribution over possible actions**:\n",
    "$$\\pi(a|s)=P(A=a|S=s)$$\n",
    "where the policy for taking action $a$ in state $s$ is equal to the probability of taking action $a$ in state $s$. \n",
    "\n",
    "We see from our Q-value table that some states have multiple actions with non-zero Q-values. Let's take state $(2,2)$ as an example. \n",
    "$$\\begin{align}\n",
    "\\text{up} = 7.29\\\\\n",
    "\\text{down} = 9.0\\\\\n",
    "\\text{left} = 7.29\\\\\n",
    "\\text{right} = 0\\\\\n",
    "\\end{align}$$\n",
    "\n",
    "We can **map these values to a probability distribution** by using an operation such as **softmax** (we can use other operations too) to assign them to the range $[0,1]$: \n",
    "$$P(a)=\\frac{e^{Q(a)}}{\\sum_{a}e^{Q(a)}}$$\n",
    "\n",
    "Let's see how that works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'up': 0.133, 'down': 0.734, 'left': 0.133, 'right': 0.0}\n"
     ]
    }
   ],
   "source": [
    "qvalues_22 = {'up': 7.29, 'down': 9.0, 'left': 7.29, 'right': 0.0}\n",
    "\n",
    "values = np.array(list(qvalues_22.values()))\n",
    "exp = np.exp(values)\n",
    "probabilities = (exp / np.sum(exp)).tolist()\n",
    "\n",
    "print({action: round(prob,3) for action, prob in zip(qvalues_22.keys(), probabilities)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our agent has a probability distribution over actions in state $(2,2)$: \n",
    "$$\\begin{align}\n",
    "\\text{up} = 13.3\\%\\\\\n",
    "\\text{down} = 73.4\\%\\\\\n",
    "\\text{left} = 13.3\\%\\\\\n",
    "\\text{right} = 0\\%\\\\\n",
    "\\end{align}$$\n",
    "\n",
    "In each state, our agent will sample an action from the action distribution probabilistically. **This will change our expected return**. The expected return will now become a **weighted sum over all possible actions**, rather than just the max-action we have seen previously. We'll explore this in more depth in another notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review\n",
    "In this notebook, we've introduced some new foundational concepts in reinforcement learning: \n",
    "- **State**: \n",
    "- **Expected return**: \n",
    "- **State-action values (Q-values)**: \n",
    "- **Markov Decision Processes (MDPs)**: \n",
    "- **Solving MDPs using the Bellman Equation and Dynamic Programming**: \n",
    "- **Policies**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
